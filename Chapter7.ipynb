{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : (データ数, チャンネル, 高さ, 幅)の4次元配列からなる入力データ\n",
    "    filter_h : フィルターの高さ\n",
    "    filter_w : フィルターの幅\n",
    "    stride : ストライド\n",
    "    pad : パディング\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2次元配列\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "#from common.util import im2col\n",
    "import numpy as np\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7)  # batch_size, channel_size, height, width\n",
    "coll = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(coll.shape)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7)  # batch_size, channel_size, height, width\n",
    "coll = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(coll.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    \n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T  \n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)  #?\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (H - self.pool_w) / self.stride)\n",
    "        \n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad) #?\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2) \n",
    "        \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"単純なConvNet\n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 入力サイズ（MNISTの場合は784）\n",
    "    hidden_size_list : 隠れ層のニューロンの数のリスト（e.g. [100, 100, 100]）\n",
    "    output_size : 出力サイズ（MNISTの場合は10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
    "        'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
    "        'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"損失関数を求める\n",
    "        引数のxは入力データ、tは教師ラベル\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（数値微分）\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（誤差逆伝搬法）\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]            \n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.300218255312933\n",
      "=== epoch:1, train acc:0.15, test acc:0.161 ===\n",
      "train loss:2.29927836173963\n",
      "train loss:2.296413369259719\n",
      "train loss:2.290504981867575\n",
      "train loss:2.2865199325603482\n",
      "train loss:2.274215329064529\n",
      "train loss:2.2680677329215597\n",
      "train loss:2.2514186847925814\n",
      "train loss:2.2337134507478456\n",
      "train loss:2.1955536532100637\n",
      "train loss:2.174728509012122\n",
      "train loss:2.146642062717053\n",
      "train loss:2.137976427301828\n",
      "train loss:2.0495799208260066\n",
      "train loss:2.020381997730619\n",
      "train loss:2.0207880899816026\n",
      "train loss:1.9340106472312848\n",
      "train loss:1.8985918590529025\n",
      "train loss:1.820968046752658\n",
      "train loss:1.748558384994067\n",
      "train loss:1.5919936993370023\n",
      "train loss:1.5788188888641972\n",
      "train loss:1.5633299276194936\n",
      "train loss:1.459459430954679\n",
      "train loss:1.3383741408260659\n",
      "train loss:1.2937565416857069\n",
      "train loss:1.2013419913359749\n",
      "train loss:1.1957436723557473\n",
      "train loss:1.0337998457796553\n",
      "train loss:1.0880988199130741\n",
      "train loss:0.971008183286483\n",
      "train loss:0.8955864512811088\n",
      "train loss:0.8567346016174047\n",
      "train loss:1.0157704908883771\n",
      "train loss:0.881577690129179\n",
      "train loss:0.9509874521037548\n",
      "train loss:0.7899357073528068\n",
      "train loss:0.8109913695379554\n",
      "train loss:0.8521688357122592\n",
      "train loss:0.65728539601118\n",
      "train loss:0.7618964038941306\n",
      "train loss:0.6484433527262979\n",
      "train loss:0.6969773172547198\n",
      "train loss:0.6025188225883538\n",
      "train loss:0.5233580111909731\n",
      "train loss:0.6374433820224187\n",
      "train loss:0.4645897625200832\n",
      "train loss:0.5972709259853227\n",
      "train loss:0.4563630371631806\n",
      "train loss:0.5092578308175336\n",
      "train loss:0.6012104784234156\n",
      "=== epoch:2, train acc:0.814, test acc:0.805 ===\n",
      "train loss:0.4802717648988458\n",
      "train loss:0.46429573509126326\n",
      "train loss:0.5571809927223073\n",
      "train loss:0.6633396381304609\n",
      "train loss:0.4352136544520643\n",
      "train loss:0.4859556641858418\n",
      "train loss:0.40537417852763646\n",
      "train loss:0.4281825834179825\n",
      "train loss:0.6840701443637596\n",
      "train loss:0.5138989634050128\n",
      "train loss:0.4935578164731899\n",
      "train loss:0.37026933380937416\n",
      "train loss:0.5117357355092382\n",
      "train loss:0.4779846174503414\n",
      "train loss:0.4125116965022266\n",
      "train loss:0.3155856793385189\n",
      "train loss:0.46032839470125997\n",
      "train loss:0.4187668631248574\n",
      "train loss:0.27870217463749536\n",
      "train loss:0.27144324404221404\n",
      "train loss:0.3138065660029531\n",
      "train loss:0.40153762255720293\n",
      "train loss:0.45897209260080274\n",
      "train loss:0.38097685095957895\n",
      "train loss:0.30414005669131716\n",
      "train loss:0.3523364785001234\n",
      "train loss:0.3252496169355993\n",
      "train loss:0.29331467979745457\n",
      "train loss:0.2057286482815636\n",
      "train loss:0.3696065899074965\n",
      "train loss:0.4893073422273978\n",
      "train loss:0.3479956774010624\n",
      "train loss:0.4049436074404049\n",
      "train loss:0.3537702001627694\n",
      "train loss:0.37483213981295394\n",
      "train loss:0.23990548341531934\n",
      "train loss:0.35826041467474007\n",
      "train loss:0.39490262479985994\n",
      "train loss:0.34601850985643956\n",
      "train loss:0.4438057547770441\n",
      "train loss:0.35420677910187076\n",
      "train loss:0.47932530414627095\n",
      "train loss:0.2660058961516052\n",
      "train loss:0.33397462101648173\n",
      "train loss:0.5741005454441833\n",
      "train loss:0.18341853492818605\n",
      "train loss:0.37345038358266325\n",
      "train loss:0.32261984429620744\n",
      "train loss:0.3622931270726777\n",
      "train loss:0.31360910636104605\n",
      "=== epoch:3, train acc:0.877, test acc:0.876 ===\n",
      "train loss:0.23259270076747762\n",
      "train loss:0.21877176172817897\n",
      "train loss:0.25132682995110234\n",
      "train loss:0.3090659344536614\n",
      "train loss:0.3212971317287788\n",
      "train loss:0.23847163142393665\n",
      "train loss:0.40609300266977344\n",
      "train loss:0.2914596445629124\n",
      "train loss:0.46953227491428895\n",
      "train loss:0.15935951260732206\n",
      "train loss:0.393286496154758\n",
      "train loss:0.24141671623594746\n",
      "train loss:0.39642163602491787\n",
      "train loss:0.2618524595300341\n",
      "train loss:0.40609571221016294\n",
      "train loss:0.3830447086030133\n",
      "train loss:0.3044491066251782\n",
      "train loss:0.29600848595013696\n",
      "train loss:0.26246112734435323\n",
      "train loss:0.1698471961980882\n",
      "train loss:0.3528666691754023\n",
      "train loss:0.40526443964055253\n",
      "train loss:0.31614454518603213\n",
      "train loss:0.40134981578823237\n",
      "train loss:0.2468629176784636\n",
      "train loss:0.2721649320724582\n",
      "train loss:0.3592947497998968\n",
      "train loss:0.25932331178545204\n",
      "train loss:0.2719497289218623\n",
      "train loss:0.36291776244585444\n",
      "train loss:0.2636506535784276\n",
      "train loss:0.23695709175849905\n",
      "train loss:0.537561453373214\n",
      "train loss:0.18460444482287847\n",
      "train loss:0.1726774319124698\n",
      "train loss:0.37490920947635914\n",
      "train loss:0.2320281363047904\n",
      "train loss:0.24591194401797378\n",
      "train loss:0.34389874416263183\n",
      "train loss:0.273496955891766\n",
      "train loss:0.299104139982248\n",
      "train loss:0.16281425647213502\n",
      "train loss:0.3033445332705612\n",
      "train loss:0.22117632796181985\n",
      "train loss:0.12492925788064536\n",
      "train loss:0.40805678099531884\n",
      "train loss:0.39839020400986713\n",
      "train loss:0.4126140257430073\n",
      "train loss:0.2827745429732317\n",
      "train loss:0.1843450645376456\n",
      "=== epoch:4, train acc:0.918, test acc:0.904 ===\n",
      "train loss:0.2560979493921515\n",
      "train loss:0.2134442282368416\n",
      "train loss:0.19337208161785743\n",
      "train loss:0.27782201918447086\n",
      "train loss:0.26175090910356874\n",
      "train loss:0.25514473151055184\n",
      "train loss:0.13868378152413013\n",
      "train loss:0.3899512455089175\n",
      "train loss:0.26777871518635377\n",
      "train loss:0.2955891732371615\n",
      "train loss:0.2791960509235347\n",
      "train loss:0.2760682754600211\n",
      "train loss:0.21162570565821667\n",
      "train loss:0.1305661653162904\n",
      "train loss:0.23921215481689864\n",
      "train loss:0.2526865993879379\n",
      "train loss:0.21239234817311792\n",
      "train loss:0.17864518298430404\n",
      "train loss:0.2072190060270518\n",
      "train loss:0.300862991409881\n",
      "train loss:0.12041485832002033\n",
      "train loss:0.2301750593489184\n",
      "train loss:0.15188051740216463\n",
      "train loss:0.3002581999820854\n",
      "train loss:0.24202771787841043\n",
      "train loss:0.24303743052399554\n",
      "train loss:0.2306259588896285\n",
      "train loss:0.18305212693987716\n",
      "train loss:0.16802922742979678\n",
      "train loss:0.20682527036704376\n",
      "train loss:0.10904538231133362\n",
      "train loss:0.3282795602982222\n",
      "train loss:0.26091872166804786\n",
      "train loss:0.14785160328895103\n",
      "train loss:0.18762533556516187\n",
      "train loss:0.32349861482650577\n",
      "train loss:0.2575845549224469\n",
      "train loss:0.2816719185778894\n",
      "train loss:0.13739431690844542\n",
      "train loss:0.17898274879557657\n",
      "train loss:0.24217739505208585\n",
      "train loss:0.22528344847598786\n",
      "train loss:0.3031259917558613\n",
      "train loss:0.2197445178352542\n",
      "train loss:0.2188033672270525\n",
      "train loss:0.3130522635522049\n",
      "train loss:0.12285862004228498\n",
      "train loss:0.14449852211136918\n",
      "train loss:0.20833780778813568\n",
      "train loss:0.29173313608739365\n",
      "=== epoch:5, train acc:0.932, test acc:0.904 ===\n",
      "train loss:0.18789984431177437\n",
      "train loss:0.20338178877018792\n",
      "train loss:0.14525209016515958\n",
      "train loss:0.20743172705618498\n",
      "train loss:0.23966121379152644\n",
      "train loss:0.250441322686372\n",
      "train loss:0.3414395546547404\n",
      "train loss:0.21976144753001667\n",
      "train loss:0.16147642318401945\n",
      "train loss:0.11973291251977969\n",
      "train loss:0.16693482501684923\n",
      "train loss:0.1657505971374996\n",
      "train loss:0.21426908494116303\n",
      "train loss:0.3096093961945181\n",
      "train loss:0.11966741927649592\n",
      "train loss:0.23223527993564136\n",
      "train loss:0.12150509482700718\n",
      "train loss:0.1932658589020012\n",
      "train loss:0.16786009999396348\n",
      "train loss:0.1162454959491956\n",
      "train loss:0.16235171056698344\n",
      "train loss:0.2532756213308097\n",
      "train loss:0.2064740766145039\n",
      "train loss:0.1385637595594399\n",
      "train loss:0.23342225412033923\n",
      "train loss:0.1073826069411412\n",
      "train loss:0.16979034710219099\n",
      "train loss:0.22043901869501162\n",
      "train loss:0.13108209366685442\n",
      "train loss:0.21925553662965058\n",
      "train loss:0.11238748529272964\n",
      "train loss:0.23086690855072375\n",
      "train loss:0.06496875105591232\n",
      "train loss:0.17890662701812793\n",
      "train loss:0.14246087878213134\n",
      "train loss:0.17430379819954203\n",
      "train loss:0.13840407779157318\n",
      "train loss:0.14180907149724936\n",
      "train loss:0.2826619340327871\n",
      "train loss:0.19829086051099765\n",
      "train loss:0.21663440260806277\n",
      "train loss:0.16899116961112276\n",
      "train loss:0.2561417327774942\n",
      "train loss:0.1445967414291907\n",
      "train loss:0.24428072935320738\n",
      "train loss:0.17193444288788778\n",
      "train loss:0.290460657406948\n",
      "train loss:0.18980256771544396\n",
      "train loss:0.14341000889377808\n",
      "train loss:0.3407599944402019\n",
      "=== epoch:6, train acc:0.941, test acc:0.917 ===\n",
      "train loss:0.14223223299373755\n",
      "train loss:0.11910032388623958\n",
      "train loss:0.16531133577186605\n",
      "train loss:0.17325076449897447\n",
      "train loss:0.1289550468269827\n",
      "train loss:0.13832283711747603\n",
      "train loss:0.159015332652651\n",
      "train loss:0.21267008894773973\n",
      "train loss:0.15625319697866305\n",
      "train loss:0.17940727997941872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0994568960028025\n",
      "train loss:0.230213863417008\n",
      "train loss:0.21928722929576996\n",
      "train loss:0.13824009281068278\n",
      "train loss:0.10713886796151291\n",
      "train loss:0.13381860656171995\n",
      "train loss:0.14701912704702483\n",
      "train loss:0.1609184132505966\n",
      "train loss:0.15361084813216136\n",
      "train loss:0.08882743316047206\n",
      "train loss:0.17080930082996054\n",
      "train loss:0.09812942440978903\n",
      "train loss:0.10890621682904543\n",
      "train loss:0.06731445276037762\n",
      "train loss:0.16335418182952618\n",
      "train loss:0.09674343100999679\n",
      "train loss:0.20379464375294704\n",
      "train loss:0.1087462046649431\n",
      "train loss:0.10516331615468806\n",
      "train loss:0.18332479462062448\n",
      "train loss:0.12415638632201949\n",
      "train loss:0.1382895406311007\n",
      "train loss:0.08455896220321066\n",
      "train loss:0.1659161413689455\n",
      "train loss:0.064862485665254\n",
      "train loss:0.09965898976588812\n",
      "train loss:0.10026402691045211\n",
      "train loss:0.24317298103367738\n",
      "train loss:0.13043253611022443\n",
      "train loss:0.17297353706336388\n",
      "train loss:0.13609749107947866\n",
      "train loss:0.24268038966417468\n",
      "train loss:0.1431390248645061\n",
      "train loss:0.1066750760212086\n",
      "train loss:0.22105544580333705\n",
      "train loss:0.19087423498510758\n",
      "train loss:0.13338889653618852\n",
      "train loss:0.14877095911903793\n",
      "train loss:0.13863841664975693\n",
      "train loss:0.13027017057721926\n",
      "=== epoch:7, train acc:0.953, test acc:0.931 ===\n",
      "train loss:0.1455863671153341\n",
      "train loss:0.0733476887311461\n",
      "train loss:0.07402380374346632\n",
      "train loss:0.12027055130560126\n",
      "train loss:0.14581664585539167\n",
      "train loss:0.10012280329134109\n",
      "train loss:0.08074350786332046\n",
      "train loss:0.10960967465938688\n",
      "train loss:0.21958811633818903\n",
      "train loss:0.08107548197603333\n",
      "train loss:0.13237689316988044\n",
      "train loss:0.04437364876419645\n",
      "train loss:0.09145836330894758\n",
      "train loss:0.20767166971716322\n",
      "train loss:0.16595093642632885\n",
      "train loss:0.0992432496898966\n",
      "train loss:0.10292246484539472\n",
      "train loss:0.0838105285026818\n",
      "train loss:0.14665002305034797\n",
      "train loss:0.1504055351458354\n",
      "train loss:0.09581943811335128\n",
      "train loss:0.13160397370182753\n",
      "train loss:0.21223323512833922\n",
      "train loss:0.20306777774158902\n",
      "train loss:0.1447370175298036\n",
      "train loss:0.11175483705737765\n",
      "train loss:0.11401595709385338\n",
      "train loss:0.16192246605711397\n",
      "train loss:0.07479621589315014\n",
      "train loss:0.14066066984789763\n",
      "train loss:0.21488473738913\n",
      "train loss:0.07347166513632755\n",
      "train loss:0.07345892578896565\n",
      "train loss:0.14071173579609364\n",
      "train loss:0.13589147156589396\n",
      "train loss:0.07936264837174822\n",
      "train loss:0.1463157950549223\n",
      "train loss:0.08101060906415421\n",
      "train loss:0.15427571896251857\n",
      "train loss:0.09818365607663303\n",
      "train loss:0.2484093114794477\n",
      "train loss:0.1355219748977823\n",
      "train loss:0.08704363825848213\n",
      "train loss:0.1005161577562048\n",
      "train loss:0.1410871913199372\n",
      "train loss:0.08518395842788279\n",
      "train loss:0.08308345706266877\n",
      "train loss:0.1273401579329008\n",
      "train loss:0.3293301050715241\n",
      "train loss:0.12945333870351416\n",
      "=== epoch:8, train acc:0.96, test acc:0.937 ===\n",
      "train loss:0.07348380819986157\n",
      "train loss:0.09693286840424074\n",
      "train loss:0.22955764149283944\n",
      "train loss:0.12341407414407687\n",
      "train loss:0.09054508957818759\n",
      "train loss:0.12996367921618837\n",
      "train loss:0.0996887670445281\n",
      "train loss:0.10970574476683766\n",
      "train loss:0.12233413960554508\n",
      "train loss:0.1143840315478996\n",
      "train loss:0.09292092548126397\n",
      "train loss:0.07333891398334128\n",
      "train loss:0.06295510562686318\n",
      "train loss:0.09130193361405528\n",
      "train loss:0.2696728604869312\n",
      "train loss:0.21609447123258302\n",
      "train loss:0.09386976008954585\n",
      "train loss:0.10352934973210356\n",
      "train loss:0.09588983318223301\n",
      "train loss:0.10047974898058103\n",
      "train loss:0.09187219077697474\n",
      "train loss:0.14346492232979907\n",
      "train loss:0.09629522180710498\n",
      "train loss:0.12530676363082907\n",
      "train loss:0.11818995384583236\n",
      "train loss:0.07604965356043607\n",
      "train loss:0.12277476641642243\n",
      "train loss:0.16891236211962535\n",
      "train loss:0.11826448960536635\n",
      "train loss:0.09629651517178647\n",
      "train loss:0.07069419786619185\n",
      "train loss:0.08685225508418179\n",
      "train loss:0.08026782742373013\n",
      "train loss:0.12163167051453623\n",
      "train loss:0.0924847401985393\n",
      "train loss:0.08571682614429996\n",
      "train loss:0.07591939721590506\n",
      "train loss:0.08494809065491808\n",
      "train loss:0.06410549270205648\n",
      "train loss:0.11254418622968845\n",
      "train loss:0.13770505437505282\n",
      "train loss:0.06209681119452459\n",
      "train loss:0.09031678473611576\n",
      "train loss:0.09291339546421241\n",
      "train loss:0.24015203443415628\n",
      "train loss:0.08110779949974288\n",
      "train loss:0.07679635198791118\n",
      "train loss:0.042333660284640987\n",
      "train loss:0.10437597565041168\n",
      "train loss:0.15430431478953957\n",
      "=== epoch:9, train acc:0.959, test acc:0.944 ===\n",
      "train loss:0.15438149885940772\n",
      "train loss:0.11958255814326853\n",
      "train loss:0.09655002496412703\n",
      "train loss:0.1934954626784711\n",
      "train loss:0.10248219040015792\n",
      "train loss:0.13593261350796998\n",
      "train loss:0.12087277546090786\n",
      "train loss:0.05258559005749612\n",
      "train loss:0.20688340286880696\n",
      "train loss:0.12197349303371427\n",
      "train loss:0.08502250317492271\n",
      "train loss:0.07989963092316466\n",
      "train loss:0.028183823265657083\n",
      "train loss:0.09912896576391696\n",
      "train loss:0.06590965807627039\n",
      "train loss:0.15866745205528188\n",
      "train loss:0.16033793137958166\n",
      "train loss:0.0636619827836661\n",
      "train loss:0.10431635742708725\n",
      "train loss:0.12363137269864163\n",
      "train loss:0.16338484618055837\n",
      "train loss:0.10315797723492129\n",
      "train loss:0.1611491750526229\n",
      "train loss:0.04176994706415406\n",
      "train loss:0.09156926404442918\n",
      "train loss:0.12107599822348585\n",
      "train loss:0.07104244614084963\n",
      "train loss:0.0697878957182852\n",
      "train loss:0.13850847397005794\n",
      "train loss:0.08041323560668491\n",
      "train loss:0.1499492080914016\n",
      "train loss:0.061978599607811956\n",
      "train loss:0.08175142543690703\n",
      "train loss:0.06828972081110905\n",
      "train loss:0.0362196821445489\n",
      "train loss:0.13237863873517258\n",
      "train loss:0.04971600697548153\n",
      "train loss:0.09444904700848074\n",
      "train loss:0.0885396208832163\n",
      "train loss:0.0938741335488993\n",
      "train loss:0.04660585162297057\n",
      "train loss:0.1924234337285938\n",
      "train loss:0.08401226406659314\n",
      "train loss:0.09652472131607581\n",
      "train loss:0.03190754930094596\n",
      "train loss:0.07349978450845991\n",
      "train loss:0.07606600638945328\n",
      "train loss:0.06705467912322198\n",
      "train loss:0.06330804043723365\n",
      "train loss:0.10266131160180544\n",
      "=== epoch:10, train acc:0.962, test acc:0.95 ===\n",
      "train loss:0.09209321932556888\n",
      "train loss:0.04803902643957421\n",
      "train loss:0.07536438562359607\n",
      "train loss:0.04334106452788089\n",
      "train loss:0.08744424042636116\n",
      "train loss:0.08684578531197484\n",
      "train loss:0.0643572611346288\n",
      "train loss:0.03829390178692158\n",
      "train loss:0.08787158050051104\n",
      "train loss:0.06571839402888269\n",
      "train loss:0.06715824312200691\n",
      "train loss:0.09565528284186188\n",
      "train loss:0.05591287766087095\n",
      "train loss:0.028840655629171198\n",
      "train loss:0.05215255614075859\n",
      "train loss:0.0438764169232879\n",
      "train loss:0.07858612112592409\n",
      "train loss:0.027092408847233643\n",
      "train loss:0.04701159654444715\n",
      "train loss:0.18206945250190795\n",
      "train loss:0.14575278659351643\n",
      "train loss:0.027322788559365044\n",
      "train loss:0.07289004065243193\n",
      "train loss:0.10544726030366311\n",
      "train loss:0.1645521390306752\n",
      "train loss:0.08539698148439817\n",
      "train loss:0.04299152292854923\n",
      "train loss:0.05930799686694419\n",
      "train loss:0.06795499029443557\n",
      "train loss:0.07626790057324738\n",
      "train loss:0.1514715038956133\n",
      "train loss:0.04223187754830226\n",
      "train loss:0.038578315565890704\n",
      "train loss:0.06786106167642549\n",
      "train loss:0.056882805350744546\n",
      "train loss:0.05196816810674731\n",
      "train loss:0.060476666902881275\n",
      "train loss:0.0480128151465038\n",
      "train loss:0.0421443242292023\n",
      "train loss:0.10369086007692788\n",
      "train loss:0.12343158091016675\n",
      "train loss:0.047819749982047535\n",
      "train loss:0.1544732175718695\n",
      "train loss:0.06971056374630775\n",
      "train loss:0.061212456541825534\n",
      "train loss:0.07332472130362905\n",
      "train loss:0.1427544172313795\n",
      "train loss:0.04950658907044637\n",
      "train loss:0.04515488942563004\n",
      "train loss:0.0467605008896865\n",
      "=== epoch:11, train acc:0.97, test acc:0.954 ===\n",
      "train loss:0.0653268064194845\n",
      "train loss:0.0427595947187676\n",
      "train loss:0.0640004569034803\n",
      "train loss:0.05289168894892711\n",
      "train loss:0.10934963133872609\n",
      "train loss:0.04763232400366477\n",
      "train loss:0.15901855996093037\n",
      "train loss:0.09543429451861343\n",
      "train loss:0.060220914355632225\n",
      "train loss:0.042173093587933794\n",
      "train loss:0.053019702066598076\n",
      "train loss:0.0397532206659216\n",
      "train loss:0.10279470684601649\n",
      "train loss:0.15459753023080178\n",
      "train loss:0.03922678814708249\n",
      "train loss:0.06406091647795047\n",
      "train loss:0.05399607531061048\n",
      "train loss:0.09684997123776318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.08803776503742884\n",
      "train loss:0.14039570834517984\n",
      "train loss:0.09069626940464247\n",
      "train loss:0.15558955402443994\n",
      "train loss:0.039125311914264736\n",
      "train loss:0.04486645259751361\n",
      "train loss:0.09113825241533141\n",
      "train loss:0.035356775997581004\n",
      "train loss:0.06880614454016479\n",
      "train loss:0.06921315985894023\n",
      "train loss:0.07826604991855747\n",
      "train loss:0.1202663707852435\n",
      "train loss:0.05238518379780129\n",
      "train loss:0.0752143864090019\n",
      "train loss:0.07762711435990857\n",
      "train loss:0.0372201167411783\n",
      "train loss:0.04186004098467198\n",
      "train loss:0.056395345221922764\n",
      "train loss:0.0549158612684015\n",
      "train loss:0.06970833589357821\n",
      "train loss:0.07284945440996012\n",
      "train loss:0.0242175058975188\n",
      "train loss:0.06579289292078362\n",
      "train loss:0.060958360338282086\n",
      "train loss:0.04355071304827592\n",
      "train loss:0.06277409453697248\n",
      "train loss:0.05796177168375746\n",
      "train loss:0.03449001957135342\n",
      "train loss:0.04466634898712253\n",
      "train loss:0.062034846250238626\n",
      "train loss:0.047818451269320134\n",
      "train loss:0.044530178712184273\n",
      "=== epoch:12, train acc:0.977, test acc:0.951 ===\n",
      "train loss:0.08330353004230337\n",
      "train loss:0.05197510288781855\n",
      "train loss:0.04072203102384796\n",
      "train loss:0.10577804880893994\n",
      "train loss:0.03613106825583901\n",
      "train loss:0.043758118104426\n",
      "train loss:0.15316756062217568\n",
      "train loss:0.06779163811076656\n",
      "train loss:0.02293788877666549\n",
      "train loss:0.04196263052566648\n",
      "train loss:0.06213683130431387\n",
      "train loss:0.08300966171646293\n",
      "train loss:0.035912973396472966\n",
      "train loss:0.05606744072423481\n",
      "train loss:0.07826929601359946\n",
      "train loss:0.03215348882283397\n",
      "train loss:0.045038214038871274\n",
      "train loss:0.03579479536927392\n",
      "train loss:0.07113510091825637\n",
      "train loss:0.08208888767570938\n",
      "train loss:0.06192937153669567\n",
      "train loss:0.05898049270494684\n",
      "train loss:0.0663106171893136\n",
      "train loss:0.04544509343661343\n",
      "train loss:0.01765091226397285\n",
      "train loss:0.08908781894798022\n",
      "train loss:0.06538982241884744\n",
      "train loss:0.06377688379874077\n",
      "train loss:0.05778092476123192\n",
      "train loss:0.04921293689748528\n",
      "train loss:0.048749743620771516\n",
      "train loss:0.047669008493132485\n",
      "train loss:0.05603656039564327\n",
      "train loss:0.00841360479438801\n",
      "train loss:0.0625299989923079\n",
      "train loss:0.06785513081246691\n",
      "train loss:0.05557992236885008\n",
      "train loss:0.021050630083041283\n",
      "train loss:0.03509655392969822\n",
      "train loss:0.034642676517201966\n",
      "train loss:0.05897874730232679\n",
      "train loss:0.07261328268995978\n",
      "train loss:0.13128190124149458\n",
      "train loss:0.019970686252482703\n",
      "train loss:0.0567083113032561\n",
      "train loss:0.041048127141040495\n",
      "train loss:0.041695279437222094\n",
      "train loss:0.024133058646691158\n",
      "train loss:0.039157911184558636\n",
      "train loss:0.06986388176267931\n",
      "=== epoch:13, train acc:0.978, test acc:0.955 ===\n",
      "train loss:0.03752361804216298\n",
      "train loss:0.0631137502904547\n",
      "train loss:0.048596054104186305\n",
      "train loss:0.04456491473812418\n",
      "train loss:0.05551958250256578\n",
      "train loss:0.18129697014219098\n",
      "train loss:0.05406073602844914\n",
      "train loss:0.037881404212538355\n",
      "train loss:0.06014304969554053\n",
      "train loss:0.030069044840409176\n",
      "train loss:0.06784027817830608\n",
      "train loss:0.12593008796023686\n",
      "train loss:0.06098492019493876\n",
      "train loss:0.07023807963853106\n",
      "train loss:0.08987745018318954\n",
      "train loss:0.04158167356139316\n",
      "train loss:0.04232793016188536\n",
      "train loss:0.048594580192034685\n",
      "train loss:0.05172571249463062\n",
      "train loss:0.03689596549305283\n",
      "train loss:0.09211474250628379\n",
      "train loss:0.06027117287193574\n",
      "train loss:0.05974859399663058\n",
      "train loss:0.08218751335641032\n",
      "train loss:0.053959421041236376\n",
      "train loss:0.03295249339247713\n",
      "train loss:0.031068311394413652\n",
      "train loss:0.01273679370389024\n",
      "train loss:0.02803563570743535\n",
      "train loss:0.06682258042546375\n",
      "train loss:0.03159035700923365\n",
      "train loss:0.03480932983247078\n",
      "train loss:0.09229491626517781\n",
      "train loss:0.031052089928341613\n",
      "train loss:0.08264555763848408\n",
      "train loss:0.035540066945649346\n",
      "train loss:0.040735344825082544\n",
      "train loss:0.023313042612471156\n",
      "train loss:0.04742419470296426\n",
      "train loss:0.02731176531887674\n",
      "train loss:0.06877473668813275\n",
      "train loss:0.06804836240064238\n",
      "train loss:0.044103340162837006\n",
      "train loss:0.03810977518691693\n",
      "train loss:0.02974799405040829\n",
      "train loss:0.03235201396593164\n",
      "train loss:0.03869078214025229\n",
      "train loss:0.03509386030933895\n",
      "train loss:0.04111757224371514\n",
      "train loss:0.047395144921188924\n",
      "=== epoch:14, train acc:0.984, test acc:0.956 ===\n",
      "train loss:0.0552990570627299\n",
      "train loss:0.08649543213715855\n",
      "train loss:0.07084770044810075\n",
      "train loss:0.03432666786068309\n",
      "train loss:0.07239140342551295\n",
      "train loss:0.033781056650867336\n",
      "train loss:0.01945568631854867\n",
      "train loss:0.03820914941908299\n",
      "train loss:0.03643112279813417\n",
      "train loss:0.03074809017902163\n",
      "train loss:0.10338663228106004\n",
      "train loss:0.04494638379845384\n",
      "train loss:0.05398132086576355\n",
      "train loss:0.07215277087745392\n",
      "train loss:0.10320118062619456\n",
      "train loss:0.02660247360234046\n",
      "train loss:0.0753697157181884\n",
      "train loss:0.06543382158781906\n",
      "train loss:0.055349673615718284\n",
      "train loss:0.058647880371460154\n",
      "train loss:0.04894466540407093\n",
      "train loss:0.03814320214380458\n",
      "train loss:0.03709133885292519\n",
      "train loss:0.061055315313001596\n",
      "train loss:0.07220522617811466\n",
      "train loss:0.024311278830204723\n",
      "train loss:0.029598742553593623\n",
      "train loss:0.08600288897323914\n",
      "train loss:0.02995430245347543\n",
      "train loss:0.040166045048278004\n",
      "train loss:0.03438588861111311\n",
      "train loss:0.02203087618470017\n",
      "train loss:0.07653492786531374\n",
      "train loss:0.09886007311738455\n",
      "train loss:0.06711482623240811\n",
      "train loss:0.05704918129953612\n",
      "train loss:0.03829137947206062\n",
      "train loss:0.027220540638714118\n",
      "train loss:0.05589072831538264\n",
      "train loss:0.029186324483654578\n",
      "train loss:0.02348326846548384\n",
      "train loss:0.0318513851174136\n",
      "train loss:0.07249909255944635\n",
      "train loss:0.035804080107232704\n",
      "train loss:0.025903371312837372\n",
      "train loss:0.027044779510881592\n",
      "train loss:0.032318479653066404\n",
      "train loss:0.013888148431776564\n",
      "train loss:0.029549402547542552\n",
      "train loss:0.07020391380698043\n",
      "=== epoch:15, train acc:0.986, test acc:0.954 ===\n",
      "train loss:0.02485504895522551\n",
      "train loss:0.041520644699914885\n",
      "train loss:0.018610985856578863\n",
      "train loss:0.047747706251765845\n",
      "train loss:0.04467541351857127\n",
      "train loss:0.01988159186301951\n",
      "train loss:0.03755930792063642\n",
      "train loss:0.03856213385650251\n",
      "train loss:0.011846159576191012\n",
      "train loss:0.036946865862014094\n",
      "train loss:0.04992836732170722\n",
      "train loss:0.029301849520011015\n",
      "train loss:0.07576415892193557\n",
      "train loss:0.020772334418777717\n",
      "train loss:0.013264897917306295\n",
      "train loss:0.038034465895065234\n",
      "train loss:0.02613060711853269\n",
      "train loss:0.014792276247094576\n",
      "train loss:0.020960699932238633\n",
      "train loss:0.0505570435715904\n",
      "train loss:0.03283795904216002\n",
      "train loss:0.08659821175478478\n",
      "train loss:0.030738494792037163\n",
      "train loss:0.026777894183613017\n",
      "train loss:0.037150248874277\n",
      "train loss:0.0261195617412321\n",
      "train loss:0.019615450627943332\n",
      "train loss:0.01958801570837125\n",
      "train loss:0.033836948876476564\n",
      "train loss:0.07714043811137818\n",
      "train loss:0.017800927132958128\n",
      "train loss:0.024669472405875855\n",
      "train loss:0.02488257681631177\n",
      "train loss:0.04033554835571748\n",
      "train loss:0.020505720502636868\n",
      "train loss:0.06478626247424937\n",
      "train loss:0.05125177980061861\n",
      "train loss:0.01672345023866848\n",
      "train loss:0.026945643094383104\n",
      "train loss:0.01739605770112135\n",
      "train loss:0.03293717717673974\n",
      "train loss:0.05551272338404316\n",
      "train loss:0.037503992898832016\n",
      "train loss:0.05180069046541159\n",
      "train loss:0.028416063005486592\n",
      "train loss:0.02061063475274112\n",
      "train loss:0.032697335732121996\n",
      "train loss:0.03925729319797445\n",
      "train loss:0.032389747437964175\n",
      "train loss:0.04277905905659046\n",
      "=== epoch:16, train acc:0.989, test acc:0.962 ===\n",
      "train loss:0.059559081899633944\n",
      "train loss:0.016896817284164596\n",
      "train loss:0.016955835888414\n",
      "train loss:0.037806174806923235\n",
      "train loss:0.01366351007267972\n",
      "train loss:0.022023929626512208\n",
      "train loss:0.05401559401644636\n",
      "train loss:0.026898740238189438\n",
      "train loss:0.01830762619921191\n",
      "train loss:0.016950772825209945\n",
      "train loss:0.03722264493841103\n",
      "train loss:0.013024010607405831\n",
      "train loss:0.024162921867023467\n",
      "train loss:0.021332529421188837\n",
      "train loss:0.02106114460477448\n",
      "train loss:0.035201087470368465\n",
      "train loss:0.00525458245523939\n",
      "train loss:0.03343637646719964\n",
      "train loss:0.029021774360068982\n",
      "train loss:0.009925081968414202\n",
      "train loss:0.010268359788093675\n",
      "train loss:0.014296451309075088\n",
      "train loss:0.04581393256521884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.02264986795339723\n",
      "train loss:0.050936009550656115\n",
      "train loss:0.04933464395151982\n",
      "train loss:0.027897096867170113\n",
      "train loss:0.014200072561886766\n",
      "train loss:0.10235157279140687\n",
      "train loss:0.022983762293756652\n",
      "train loss:0.010377531072193448\n",
      "train loss:0.01951537593540838\n",
      "train loss:0.03175929810248857\n",
      "train loss:0.02052884045675437\n",
      "train loss:0.013128895377017528\n",
      "train loss:0.02329116760282219\n",
      "train loss:0.01673650927224529\n",
      "train loss:0.01661397945054865\n",
      "train loss:0.029548212678101708\n",
      "train loss:0.021079372037854066\n",
      "train loss:0.024053195277478418\n",
      "train loss:0.027883372338187565\n",
      "train loss:0.02748867900421453\n",
      "train loss:0.034713488014460725\n",
      "train loss:0.010436110679437078\n",
      "train loss:0.016290958076090704\n",
      "train loss:0.007158990843750958\n",
      "train loss:0.019200805798611605\n",
      "train loss:0.028159446324734595\n",
      "train loss:0.07048130483899032\n",
      "=== epoch:17, train acc:0.993, test acc:0.96 ===\n",
      "train loss:0.03025612227419476\n",
      "train loss:0.02302927746264303\n",
      "train loss:0.028571227661199825\n",
      "train loss:0.019913794722165472\n",
      "train loss:0.03860426435575022\n",
      "train loss:0.06292417014548687\n",
      "train loss:0.05318996229546744\n",
      "train loss:0.018949156661182572\n",
      "train loss:0.011939456295742225\n",
      "train loss:0.025439553117219314\n",
      "train loss:0.04509614739008782\n",
      "train loss:0.03957353904874082\n",
      "train loss:0.007645601458045487\n",
      "train loss:0.06571737844691856\n",
      "train loss:0.015978213373752638\n",
      "train loss:0.03211621645322928\n",
      "train loss:0.023170258916209857\n",
      "train loss:0.043977706336961825\n",
      "train loss:0.011787040369948305\n",
      "train loss:0.020228270577058168\n",
      "train loss:0.032644321635624376\n",
      "train loss:0.013600811621065158\n",
      "train loss:0.03600757143198491\n",
      "train loss:0.018163477528051426\n",
      "train loss:0.01983776934891393\n",
      "train loss:0.03134614596021486\n",
      "train loss:0.01146306254570919\n",
      "train loss:0.01155976282616772\n",
      "train loss:0.017298139012217842\n",
      "train loss:0.03296590983905411\n",
      "train loss:0.05475067463914203\n",
      "train loss:0.04371034773082767\n",
      "train loss:0.03378832209632622\n",
      "train loss:0.03256167406624915\n",
      "train loss:0.026342104229859217\n",
      "train loss:0.032537368976508343\n",
      "train loss:0.06219234733015843\n",
      "train loss:0.022595121402370046\n",
      "train loss:0.0872747133475254\n",
      "train loss:0.04182317095164823\n",
      "train loss:0.015535976573286132\n",
      "train loss:0.020164880403394633\n",
      "train loss:0.02635846204163419\n",
      "train loss:0.012715947781734595\n",
      "train loss:0.035024962995435616\n",
      "train loss:0.018361158462708096\n",
      "train loss:0.013130150970303685\n",
      "train loss:0.022236923236765134\n",
      "train loss:0.01088843964068198\n",
      "train loss:0.02585339566774656\n",
      "=== epoch:18, train acc:0.991, test acc:0.954 ===\n",
      "train loss:0.09099829589131161\n",
      "train loss:0.0319986851827157\n",
      "train loss:0.036072654928756304\n",
      "train loss:0.03044336430045709\n",
      "train loss:0.018124955426270902\n",
      "train loss:0.00497066909119777\n",
      "train loss:0.019301212626945077\n",
      "train loss:0.024913816297860082\n",
      "train loss:0.025663325807602062\n",
      "train loss:0.017078897918854145\n",
      "train loss:0.01047451590799539\n",
      "train loss:0.016714748413836845\n",
      "train loss:0.009615314096538903\n",
      "train loss:0.041025692643943595\n",
      "train loss:0.061899325157118684\n",
      "train loss:0.007049466047066777\n",
      "train loss:0.01725736110255306\n",
      "train loss:0.017593589296853965\n",
      "train loss:0.008112039701147185\n",
      "train loss:0.028443788753613187\n",
      "train loss:0.054094451626404194\n",
      "train loss:0.023935356071295127\n",
      "train loss:0.022402613977643607\n",
      "train loss:0.020837351501506247\n",
      "train loss:0.02813563192955627\n",
      "train loss:0.007085552592373394\n",
      "train loss:0.02172435315562623\n",
      "train loss:0.016518699394901575\n",
      "train loss:0.018727139585697706\n",
      "train loss:0.03595459914120007\n",
      "train loss:0.007954861694928133\n",
      "train loss:0.016864397292454633\n",
      "train loss:0.01648612744069784\n",
      "train loss:0.056548654283636396\n",
      "train loss:0.018308322423886067\n",
      "train loss:0.010897005471157297\n",
      "train loss:0.02097027977694936\n",
      "train loss:0.02221610111013491\n",
      "train loss:0.03825781307262812\n",
      "train loss:0.03408284984801816\n",
      "train loss:0.01347805495403061\n",
      "train loss:0.011714479012665311\n",
      "train loss:0.013861258057820713\n",
      "train loss:0.03179937589444614\n",
      "train loss:0.028563417003103643\n",
      "train loss:0.020883211254543746\n",
      "train loss:0.01518133251656308\n",
      "train loss:0.03324916156645946\n",
      "train loss:0.02097560882336588\n",
      "train loss:0.013180819288370537\n",
      "=== epoch:19, train acc:0.997, test acc:0.961 ===\n",
      "train loss:0.010959878019822726\n",
      "train loss:0.013560341264714892\n",
      "train loss:0.018649234978412536\n",
      "train loss:0.02951044892567507\n",
      "train loss:0.01456592071716337\n",
      "train loss:0.01848865965049793\n",
      "train loss:0.012858799289634704\n",
      "train loss:0.02724612992354113\n",
      "train loss:0.045456941064672025\n",
      "train loss:0.011242500624342034\n",
      "train loss:0.01012854683278864\n",
      "train loss:0.025204179370605314\n",
      "train loss:0.009033640061984333\n",
      "train loss:0.03936691523702095\n",
      "train loss:0.04991097058574086\n",
      "train loss:0.0121706682620665\n",
      "train loss:0.047722332192088414\n",
      "train loss:0.01224433881003386\n",
      "train loss:0.043965584149512675\n",
      "train loss:0.02549680751309099\n",
      "train loss:0.015360656544722639\n",
      "train loss:0.010282745289605044\n",
      "train loss:0.014662028761745024\n",
      "train loss:0.010954102655982227\n",
      "train loss:0.023299002964177692\n",
      "train loss:0.00820618854262107\n",
      "train loss:0.019453870995456358\n",
      "train loss:0.05287388597230165\n",
      "train loss:0.03848285342180746\n",
      "train loss:0.01989174810880802\n",
      "train loss:0.011616273462706904\n",
      "train loss:0.013995915754191412\n",
      "train loss:0.026506717790072295\n",
      "train loss:0.01720693977749542\n",
      "train loss:0.023550697257382742\n",
      "train loss:0.013324115948089793\n",
      "train loss:0.015011246509037492\n",
      "train loss:0.03401119561907032\n",
      "train loss:0.010916981028201496\n",
      "train loss:0.05639211703547555\n",
      "train loss:0.007191483312657646\n",
      "train loss:0.00803253890505929\n",
      "train loss:0.018279021009445305\n",
      "train loss:0.01658134499781651\n",
      "train loss:0.010033133202143198\n",
      "train loss:0.0064856017374288165\n",
      "train loss:0.017799029318570136\n",
      "train loss:0.015993092841009613\n",
      "train loss:0.024372237596446973\n",
      "train loss:0.03522128189040992\n",
      "=== epoch:20, train acc:0.999, test acc:0.958 ===\n",
      "train loss:0.00429939705907004\n",
      "train loss:0.010269174636952266\n",
      "train loss:0.021913792356381846\n",
      "train loss:0.007714897974560441\n",
      "train loss:0.03763379148627095\n",
      "train loss:0.012022696552105644\n",
      "train loss:0.009011935509389082\n",
      "train loss:0.01986699708474258\n",
      "train loss:0.01894756180510473\n",
      "train loss:0.019985298865852125\n",
      "train loss:0.007767996897720939\n",
      "train loss:0.0158840177619149\n",
      "train loss:0.0050733957304244385\n",
      "train loss:0.008868266315520708\n",
      "train loss:0.008828212754626683\n",
      "train loss:0.02773730517725551\n",
      "train loss:0.011582106374494288\n",
      "train loss:0.015239202262831155\n",
      "train loss:0.01750511841228232\n",
      "train loss:0.004909904166690483\n",
      "train loss:0.017864408528223352\n",
      "train loss:0.008219904135762332\n",
      "train loss:0.012129282763683292\n",
      "train loss:0.008266399139859538\n",
      "train loss:0.007426358373990144\n",
      "train loss:0.01120499109552751\n",
      "train loss:0.007508624479318101\n",
      "train loss:0.004406754335182829\n",
      "train loss:0.04221056705968484\n",
      "train loss:0.004243060201199065\n",
      "train loss:0.010638694125850515\n",
      "train loss:0.01285281920774602\n",
      "train loss:0.011960913203437375\n",
      "train loss:0.01437896686130463\n",
      "train loss:0.010167338609144823\n",
      "train loss:0.004588910376286751\n",
      "train loss:0.006675694848821145\n",
      "train loss:0.011326551922131532\n",
      "train loss:0.02153801069050508\n",
      "train loss:0.010276472019618866\n",
      "train loss:0.02162033128234983\n",
      "train loss:0.010574665313975147\n",
      "train loss:0.011779619109574835\n",
      "train loss:0.013273675930662934\n",
      "train loss:0.019050111048656337\n",
      "train loss:0.011596063510003895\n",
      "train loss:0.02132299808794412\n",
      "train loss:0.006731000492440545\n",
      "train loss:0.014616318677149225\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.962\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "#from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c0f090d92127>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleConvNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# ランダム初期化後の重み\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mfilter_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m#print(network.params['W1'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-c0f090d92127>\u001b[0m in \u001b[0;36mfilter_show\u001b[0;34m(filters, nx, margin, scale)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mgist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgithub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0maidiary\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0md530d5e08011832b12\u001b[0m\u001b[0;31m#file-draw_weight-py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \"\"\"\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mFN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFN\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from simple_convnet import SimpleConvNet\n",
    "\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "# ランダム初期化後の重み\n",
    "#filter_show(network.params['W1'])\n",
    "print(network.params['W1'])\n",
    "\n",
    "# 学習後の重み\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
