{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : (データ数, チャンネル, 高さ, 幅)の4次元配列からなる入力データ\n",
    "    filter_h : フィルターの高さ\n",
    "    filter_w : フィルターの幅\n",
    "    stride : ストライド\n",
    "    pad : パディング\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2次元配列\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "#from common.util import im2col\n",
    "import numpy as np\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7)  # batch_size, channel_size, height, width\n",
    "coll = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(coll.shape)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7)  # batch_size, channel_size, height, width\n",
    "coll = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(coll.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    \n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T  \n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)  #?\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (H - self.pool_w) / self.stride)\n",
    "        \n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad) #?\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2) \n",
    "        \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-95935e9e0ab8>, line 150)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-95935e9e0ab8>\"\u001b[0;36m, line \u001b[0;32m150\u001b[0m\n\u001b[0;31m    self.layers[key].W = self.params['W' + str(i+1)]            self.layers[key].b = self.params['b' + str(i+1)]\u001b[0m\n\u001b[0m                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"単純なConvNet\n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 入力サイズ（MNISTの場合は784）\n",
    "    hidden_size_list : 隠れ層のニューロンの数のリスト（e.g. [100, 100, 100]）\n",
    "    output_size : 出力サイズ（MNISTの場合は10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
    "        'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
    "        'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"損失関数を求める\n",
    "        引数のxは入力データ、tは教師ラベル\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（数値微分）\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（誤差逆伝搬法）\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.30022606535\n",
      "=== epoch:1, train acc:0.233, test acc:0.223 ===\n",
      "train loss:2.29939960027\n",
      "train loss:2.29518729673\n",
      "train loss:2.28901755095\n",
      "train loss:2.28401148956\n",
      "train loss:2.2736991712\n",
      "train loss:2.25535804877\n",
      "train loss:2.24867227311\n",
      "train loss:2.22967144857\n",
      "train loss:2.19619425257\n",
      "train loss:2.20343527963\n",
      "train loss:2.13860469709\n",
      "train loss:2.10578110039\n",
      "train loss:2.05440647813\n",
      "train loss:2.01609557897\n",
      "train loss:1.96961235878\n",
      "train loss:1.89920095091\n",
      "train loss:1.82397684957\n",
      "train loss:1.77226185804\n",
      "train loss:1.71560318626\n",
      "train loss:1.56903123155\n",
      "train loss:1.56055646908\n",
      "train loss:1.4178563478\n",
      "train loss:1.41959241141\n",
      "train loss:1.25414716698\n",
      "train loss:1.31435785899\n",
      "train loss:1.17781131257\n",
      "train loss:1.06830175049\n",
      "train loss:1.02586807376\n",
      "train loss:0.925791556847\n",
      "train loss:0.796584524533\n",
      "train loss:0.651982516349\n",
      "train loss:0.800595639673\n",
      "train loss:0.660801734357\n",
      "train loss:0.723120867688\n",
      "train loss:0.71272365228\n",
      "train loss:0.527912729159\n",
      "train loss:0.756311013974\n",
      "train loss:0.551300815098\n",
      "train loss:0.498607593953\n",
      "train loss:0.641023553454\n",
      "train loss:0.490840059398\n",
      "train loss:0.541890250601\n",
      "train loss:0.559517388114\n",
      "train loss:0.47326734148\n",
      "train loss:0.558657863558\n",
      "train loss:0.623553992936\n",
      "train loss:0.571512569553\n",
      "train loss:0.582889252923\n",
      "train loss:0.478425579994\n",
      "train loss:0.434533122137\n",
      "train loss:0.518355155773\n",
      "train loss:0.497723133447\n",
      "train loss:0.573366944287\n",
      "train loss:0.522707920701\n",
      "train loss:0.475715409645\n",
      "train loss:0.842343845988\n",
      "train loss:0.544349698011\n",
      "train loss:0.525952312531\n",
      "train loss:0.457667809263\n",
      "train loss:0.476883389447\n",
      "train loss:0.638678278138\n",
      "train loss:0.558560907862\n",
      "train loss:0.344314940565\n",
      "train loss:0.482901828754\n",
      "train loss:0.60558305253\n",
      "train loss:0.455930888648\n",
      "train loss:0.401273349568\n",
      "train loss:0.454346136751\n",
      "train loss:0.441581507143\n",
      "train loss:0.404433582092\n",
      "train loss:0.496641448272\n",
      "train loss:0.469966000299\n",
      "train loss:0.326772436814\n",
      "train loss:0.696229728241\n",
      "train loss:0.47868222651\n",
      "train loss:0.425967367059\n",
      "train loss:0.444451643864\n",
      "train loss:0.61057011549\n",
      "train loss:0.425504912458\n",
      "train loss:0.435993637807\n",
      "train loss:0.407940801307\n",
      "train loss:0.339660711942\n",
      "train loss:0.360880353472\n",
      "train loss:0.338537140882\n",
      "train loss:0.453511506361\n",
      "train loss:0.436182325481\n",
      "train loss:0.382126521361\n",
      "train loss:0.542376736814\n",
      "train loss:0.332484257394\n",
      "train loss:0.554842372228\n",
      "train loss:0.333844768582\n",
      "train loss:0.423853082785\n",
      "train loss:0.551870463993\n",
      "train loss:0.384442581396\n",
      "train loss:0.305962415574\n",
      "train loss:0.477490724486\n",
      "train loss:0.463363372924\n",
      "train loss:0.499989831924\n",
      "train loss:0.33459161547\n",
      "train loss:0.227672048174\n",
      "train loss:0.389680519309\n",
      "train loss:0.333841608947\n",
      "train loss:0.503993933798\n",
      "train loss:0.412255151898\n",
      "train loss:0.285032671819\n",
      "train loss:0.452365919154\n",
      "train loss:0.53740612345\n",
      "train loss:0.307879486778\n",
      "train loss:0.484828822398\n",
      "train loss:0.473046170602\n",
      "train loss:0.254446996283\n",
      "train loss:0.291024382038\n",
      "train loss:0.38556520745\n",
      "train loss:0.281672354285\n",
      "train loss:0.327357674712\n",
      "train loss:0.462019911421\n",
      "train loss:0.386910391908\n",
      "train loss:0.415859966257\n",
      "train loss:0.358721543862\n",
      "train loss:0.425472038052\n",
      "train loss:0.258894384708\n",
      "train loss:0.382711444621\n",
      "train loss:0.351566162258\n",
      "train loss:0.29893901343\n",
      "train loss:0.393561288923\n",
      "train loss:0.307084773168\n",
      "train loss:0.386375193083\n",
      "train loss:0.308458035105\n",
      "train loss:0.350286479371\n",
      "train loss:0.366606247585\n",
      "train loss:0.190167254308\n",
      "train loss:0.338106454338\n",
      "train loss:0.323730877055\n",
      "train loss:0.365805417121\n",
      "train loss:0.515828011247\n",
      "train loss:0.416325444517\n",
      "train loss:0.457059625892\n",
      "train loss:0.255884762628\n",
      "train loss:0.409513499458\n",
      "train loss:0.342141210573\n",
      "train loss:0.183107384269\n",
      "train loss:0.247921188006\n",
      "train loss:0.212097804837\n",
      "train loss:0.242639506845\n",
      "train loss:0.29670670481\n",
      "train loss:0.311136918037\n",
      "train loss:0.591843091848\n",
      "train loss:0.343057718637\n",
      "train loss:0.328830376826\n",
      "train loss:0.371397359637\n",
      "train loss:0.242233968323\n",
      "train loss:0.371194585193\n",
      "train loss:0.209127771542\n",
      "train loss:0.403136881964\n",
      "train loss:0.287750686319\n",
      "train loss:0.246853291177\n",
      "train loss:0.207912071632\n",
      "train loss:0.317436609735\n",
      "train loss:0.214790411604\n",
      "train loss:0.411600161245\n",
      "train loss:0.252518537448\n",
      "train loss:0.327766440386\n",
      "train loss:0.25147501522\n",
      "train loss:0.32531897728\n",
      "train loss:0.36585760764\n",
      "train loss:0.430324625062\n",
      "train loss:0.416110191124\n",
      "train loss:0.251193297931\n",
      "train loss:0.333035998163\n",
      "train loss:0.332484374366\n",
      "train loss:0.229035007859\n",
      "train loss:0.229518967731\n",
      "train loss:0.2851490633\n",
      "train loss:0.32366625235\n",
      "train loss:0.390718952707\n",
      "train loss:0.191829689555\n",
      "train loss:0.339228048257\n",
      "train loss:0.214408164236\n",
      "train loss:0.263100479387\n",
      "train loss:0.349614908801\n",
      "train loss:0.450243673271\n",
      "train loss:0.321173071519\n",
      "train loss:0.284841341118\n",
      "train loss:0.270186074196\n",
      "train loss:0.351599766699\n",
      "train loss:0.20205431803\n",
      "train loss:0.254545629134\n",
      "train loss:0.231029535065\n",
      "train loss:0.214664448305\n",
      "train loss:0.321601710466\n",
      "train loss:0.348705960728\n",
      "train loss:0.23510020308\n",
      "train loss:0.199878117595\n",
      "train loss:0.309813565876\n",
      "train loss:0.267277199313\n",
      "train loss:0.207249584798\n",
      "train loss:0.375670050166\n",
      "train loss:0.293965794989\n",
      "train loss:0.47601041586\n",
      "train loss:0.297400058047\n",
      "train loss:0.272277858501\n",
      "train loss:0.281039545023\n",
      "train loss:0.379163546453\n",
      "train loss:0.204242545387\n",
      "train loss:0.331202324531\n",
      "train loss:0.188523490473\n",
      "train loss:0.174733279496\n",
      "train loss:0.25412944637\n",
      "train loss:0.260837546558\n",
      "train loss:0.244963743733\n",
      "train loss:0.334247268137\n",
      "train loss:0.393357544233\n",
      "train loss:0.280662024439\n",
      "train loss:0.342631561624\n",
      "train loss:0.123018293051\n",
      "train loss:0.305386839756\n",
      "train loss:0.191214644279\n",
      "train loss:0.191031017216\n",
      "train loss:0.273383421202\n",
      "train loss:0.443564448493\n",
      "train loss:0.347682978311\n",
      "train loss:0.355264415913\n",
      "train loss:0.250061380033\n",
      "train loss:0.205302721304\n",
      "train loss:0.309114788549\n",
      "train loss:0.311564385192\n",
      "train loss:0.302117023455\n",
      "train loss:0.233464208473\n",
      "train loss:0.464864246093\n",
      "train loss:0.371507322667\n",
      "train loss:0.161075431985\n",
      "train loss:0.309679026454\n",
      "train loss:0.16773731046\n",
      "train loss:0.147028876858\n",
      "train loss:0.28256404552\n",
      "train loss:0.181554951842\n",
      "train loss:0.333350365438\n",
      "train loss:0.135479599792\n",
      "train loss:0.314264785557\n",
      "train loss:0.45701909342\n",
      "train loss:0.164809753005\n",
      "train loss:0.294777863754\n",
      "train loss:0.277527942524\n",
      "train loss:0.161853934581\n",
      "train loss:0.157886773632\n",
      "train loss:0.236344231715\n",
      "train loss:0.185424851203\n",
      "train loss:0.279713022986\n",
      "train loss:0.232522959322\n",
      "train loss:0.302266757313\n",
      "train loss:0.170369089129\n",
      "train loss:0.187822664762\n",
      "train loss:0.175559595841\n",
      "train loss:0.168163456767\n",
      "train loss:0.255781066769\n",
      "train loss:0.192539985954\n",
      "train loss:0.242863923684\n",
      "train loss:0.135759476074\n",
      "train loss:0.177642086267\n",
      "train loss:0.345779701894\n",
      "train loss:0.213738672354\n",
      "train loss:0.174973824646\n",
      "train loss:0.172451635372\n",
      "train loss:0.183463696442\n",
      "train loss:0.270738208763\n",
      "train loss:0.191546947167\n",
      "train loss:0.348257053416\n",
      "train loss:0.269460949845\n",
      "train loss:0.369927433335\n",
      "train loss:0.261359631701\n",
      "train loss:0.270305904483\n",
      "train loss:0.166741581131\n",
      "train loss:0.28423166315\n",
      "train loss:0.172675891462\n",
      "train loss:0.193864825873\n",
      "train loss:0.128811598219\n",
      "train loss:0.149313487848\n",
      "train loss:0.205897006663\n",
      "train loss:0.217548882441\n",
      "train loss:0.192898100131\n",
      "train loss:0.416269929411\n",
      "train loss:0.166797941176\n",
      "train loss:0.237804849566\n",
      "train loss:0.266418805074\n",
      "train loss:0.306096303005\n",
      "train loss:0.230892249281\n",
      "train loss:0.211585947104\n",
      "train loss:0.187119170362\n",
      "train loss:0.0807970991662\n",
      "train loss:0.244686220351\n",
      "train loss:0.212075801411\n",
      "train loss:0.225088640427\n",
      "train loss:0.247351043108\n",
      "train loss:0.12172562717\n",
      "train loss:0.238702673221\n",
      "train loss:0.216581679045\n",
      "train loss:0.226833678798\n",
      "train loss:0.239845744138\n",
      "train loss:0.112016258564\n",
      "train loss:0.241243315995\n",
      "train loss:0.186189600237\n",
      "train loss:0.116886540917\n",
      "train loss:0.220087948994\n",
      "train loss:0.227581282567\n",
      "train loss:0.118017495157\n",
      "train loss:0.137103625999\n",
      "train loss:0.141774074261\n",
      "train loss:0.186863128587\n",
      "train loss:0.119820083444\n",
      "train loss:0.168759599228\n",
      "train loss:0.180685279587\n",
      "train loss:0.203492644123\n",
      "train loss:0.112189448591\n",
      "train loss:0.305066896412\n",
      "train loss:0.196493201737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.191296929569\n",
      "train loss:0.295282104981\n",
      "train loss:0.209872904586\n",
      "train loss:0.191763128053\n",
      "train loss:0.15700236896\n",
      "train loss:0.176147450559\n",
      "train loss:0.330009939541\n",
      "train loss:0.2103688827\n",
      "train loss:0.161217846957\n",
      "train loss:0.229261259993\n",
      "train loss:0.182834336127\n",
      "train loss:0.209428137205\n",
      "train loss:0.214959345191\n",
      "train loss:0.191248715112\n",
      "train loss:0.177145898637\n",
      "train loss:0.199832671284\n",
      "train loss:0.130380999931\n",
      "train loss:0.147767705275\n",
      "train loss:0.183753234632\n",
      "train loss:0.285270625068\n",
      "train loss:0.218649323463\n",
      "train loss:0.0956122653784\n",
      "train loss:0.329575538034\n",
      "train loss:0.258642068043\n",
      "train loss:0.088095762121\n",
      "train loss:0.155145220734\n",
      "train loss:0.258811649672\n",
      "train loss:0.142258777975\n",
      "train loss:0.170925292729\n",
      "train loss:0.148148422602\n",
      "train loss:0.182845814089\n",
      "train loss:0.23996304932\n",
      "train loss:0.26395463165\n",
      "train loss:0.0790001956186\n",
      "train loss:0.0843713538818\n",
      "train loss:0.113387571144\n",
      "train loss:0.142691209818\n",
      "train loss:0.088336505748\n",
      "train loss:0.124278904162\n",
      "train loss:0.224034036803\n",
      "train loss:0.165874820777\n",
      "train loss:0.22064725005\n",
      "train loss:0.229779328322\n",
      "train loss:0.238936501845\n",
      "train loss:0.141518819936\n",
      "train loss:0.165188779072\n",
      "train loss:0.19425000682\n",
      "train loss:0.119583951702\n",
      "train loss:0.293450562015\n",
      "train loss:0.15354785863\n",
      "train loss:0.220413055838\n",
      "train loss:0.174444566863\n",
      "train loss:0.209185502528\n",
      "train loss:0.257541728804\n",
      "train loss:0.101876784256\n",
      "train loss:0.109469451114\n",
      "train loss:0.176172794757\n",
      "train loss:0.268013940487\n",
      "train loss:0.13859823861\n",
      "train loss:0.204772941944\n",
      "train loss:0.202135379297\n",
      "train loss:0.204482651293\n",
      "train loss:0.231827363447\n",
      "train loss:0.0918981529354\n",
      "train loss:0.19495254713\n",
      "train loss:0.194273028514\n",
      "train loss:0.0774070240431\n",
      "train loss:0.140583911011\n",
      "train loss:0.126161551187\n",
      "train loss:0.165885353544\n",
      "train loss:0.14390283267\n",
      "train loss:0.189409401771\n",
      "train loss:0.129512718713\n",
      "train loss:0.230325661777\n",
      "train loss:0.0548768550431\n",
      "train loss:0.106789824631\n",
      "train loss:0.100500207714\n",
      "train loss:0.123444897036\n",
      "train loss:0.16230012669\n",
      "train loss:0.239803491808\n",
      "train loss:0.201142803068\n",
      "train loss:0.267060422175\n",
      "train loss:0.10442924696\n",
      "train loss:0.0780710208153\n",
      "train loss:0.146735201952\n",
      "train loss:0.294962975088\n",
      "train loss:0.202826187156\n",
      "train loss:0.242914236447\n",
      "train loss:0.162931895774\n",
      "train loss:0.23839304816\n",
      "train loss:0.125186911948\n",
      "train loss:0.171996151367\n",
      "train loss:0.083409800834\n",
      "train loss:0.125432768362\n",
      "train loss:0.133955653648\n",
      "train loss:0.247873834553\n",
      "train loss:0.115481788172\n",
      "train loss:0.166666907412\n",
      "train loss:0.106380479365\n",
      "train loss:0.17971248726\n",
      "train loss:0.16101673198\n",
      "train loss:0.200573315412\n",
      "train loss:0.182790451119\n",
      "train loss:0.1507744415\n",
      "train loss:0.0915397564964\n",
      "train loss:0.26438998066\n",
      "train loss:0.111198714997\n",
      "train loss:0.090952347043\n",
      "train loss:0.0902291424637\n",
      "train loss:0.0942747527886\n",
      "train loss:0.235629240485\n",
      "train loss:0.222029635408\n",
      "train loss:0.192199322163\n",
      "train loss:0.127856330339\n",
      "train loss:0.142193986565\n",
      "train loss:0.148337037832\n",
      "train loss:0.0797259869463\n",
      "train loss:0.190069629548\n",
      "train loss:0.192765612179\n",
      "train loss:0.0848421202622\n",
      "train loss:0.163881764818\n",
      "train loss:0.229513125908\n",
      "train loss:0.0957307235272\n",
      "train loss:0.133232150401\n",
      "train loss:0.175296607764\n",
      "train loss:0.101329642797\n",
      "train loss:0.0600029833485\n",
      "train loss:0.126556106699\n",
      "train loss:0.160416286902\n",
      "train loss:0.0734476216123\n",
      "train loss:0.0761745740767\n",
      "train loss:0.0588972662624\n",
      "train loss:0.109371341468\n",
      "train loss:0.121738320389\n",
      "train loss:0.215925859094\n",
      "train loss:0.0783012883218\n",
      "train loss:0.187266028585\n",
      "train loss:0.0896149545602\n",
      "train loss:0.149570823714\n",
      "train loss:0.125775719895\n",
      "train loss:0.0926540766311\n",
      "train loss:0.112024051991\n",
      "train loss:0.112547072299\n",
      "train loss:0.095936039415\n",
      "train loss:0.145569991265\n",
      "train loss:0.100475335355\n",
      "train loss:0.124664802407\n",
      "train loss:0.0950907885386\n",
      "train loss:0.149715409039\n",
      "train loss:0.296901289497\n",
      "train loss:0.161676888876\n",
      "train loss:0.151237535614\n",
      "train loss:0.20512704542\n",
      "train loss:0.117341727399\n",
      "train loss:0.170601566738\n",
      "train loss:0.180666406783\n",
      "train loss:0.169399614822\n",
      "train loss:0.0884564626371\n",
      "train loss:0.121433670899\n",
      "train loss:0.139212581807\n",
      "train loss:0.0963092535966\n",
      "train loss:0.166032648713\n",
      "train loss:0.125580619701\n",
      "train loss:0.10724306241\n",
      "train loss:0.112442551336\n",
      "train loss:0.214016449542\n",
      "train loss:0.181635531113\n",
      "train loss:0.23231479536\n",
      "train loss:0.0980445436456\n",
      "train loss:0.0605959509677\n",
      "train loss:0.1616924861\n",
      "train loss:0.105018646344\n",
      "train loss:0.127318529442\n",
      "train loss:0.127312778195\n",
      "train loss:0.112604010269\n",
      "train loss:0.0897252889664\n",
      "train loss:0.123551001461\n",
      "train loss:0.144800438784\n",
      "train loss:0.163149273865\n",
      "train loss:0.139070830909\n",
      "train loss:0.053994129799\n",
      "train loss:0.0856806686977\n",
      "train loss:0.0568974021446\n",
      "train loss:0.0809912946684\n",
      "train loss:0.148474464583\n",
      "train loss:0.0638555455628\n",
      "train loss:0.0964983393521\n",
      "train loss:0.0992334815861\n",
      "train loss:0.105451656129\n",
      "train loss:0.0921047639109\n",
      "train loss:0.183337630493\n",
      "train loss:0.123373466219\n",
      "train loss:0.0781003556686\n",
      "train loss:0.0812964127627\n",
      "train loss:0.220967922246\n",
      "train loss:0.105525661542\n",
      "train loss:0.176595673703\n",
      "train loss:0.0926460388663\n",
      "train loss:0.203208214998\n",
      "train loss:0.138579167701\n",
      "train loss:0.131496855888\n",
      "train loss:0.0664295048477\n",
      "train loss:0.123617465587\n",
      "train loss:0.154340589767\n",
      "train loss:0.131689701464\n",
      "train loss:0.0889962101568\n",
      "train loss:0.0668689458798\n",
      "train loss:0.0636990070031\n",
      "train loss:0.107525182325\n",
      "train loss:0.076579090074\n",
      "train loss:0.0808348930435\n",
      "train loss:0.0888491177062\n",
      "train loss:0.241365699677\n",
      "train loss:0.146564637625\n",
      "train loss:0.0736396765141\n",
      "train loss:0.126351160383\n",
      "train loss:0.160039941859\n",
      "train loss:0.106958466896\n",
      "train loss:0.0774960868588\n",
      "train loss:0.183207313892\n",
      "train loss:0.0934788856683\n",
      "train loss:0.127779571595\n",
      "train loss:0.168983068741\n",
      "train loss:0.0814698043979\n",
      "train loss:0.107760653757\n",
      "train loss:0.146411998688\n",
      "train loss:0.0924860621617\n",
      "train loss:0.0845012475007\n",
      "train loss:0.106428560875\n",
      "train loss:0.0632946900937\n",
      "train loss:0.087420876731\n",
      "train loss:0.150285718153\n",
      "train loss:0.0508640324375\n",
      "train loss:0.101342841608\n",
      "train loss:0.0913953268938\n",
      "train loss:0.0864117758081\n",
      "train loss:0.12134462494\n",
      "train loss:0.0858618031237\n",
      "train loss:0.188854296898\n",
      "train loss:0.0892383205317\n",
      "train loss:0.169224263592\n",
      "train loss:0.0978526145515\n",
      "train loss:0.0407983543973\n",
      "train loss:0.174576326915\n",
      "train loss:0.131352555474\n",
      "train loss:0.0857970409297\n",
      "train loss:0.0934741430101\n",
      "train loss:0.206660279888\n",
      "train loss:0.0802483781147\n",
      "train loss:0.0775965946127\n",
      "train loss:0.144262965324\n",
      "train loss:0.1269213393\n",
      "train loss:0.0700390276222\n",
      "train loss:0.0624038804676\n",
      "train loss:0.0666350505403\n",
      "train loss:0.082990379541\n",
      "train loss:0.116708079908\n",
      "train loss:0.101583249131\n",
      "train loss:0.0702236055622\n",
      "train loss:0.0991021208507\n",
      "train loss:0.108821783743\n",
      "train loss:0.0528576529364\n",
      "train loss:0.133168112177\n",
      "train loss:0.125689227565\n",
      "train loss:0.112243868167\n",
      "train loss:0.125009136839\n",
      "train loss:0.133852330429\n",
      "train loss:0.122078306269\n",
      "train loss:0.127095872334\n",
      "train loss:0.200250502964\n",
      "train loss:0.109886047237\n",
      "train loss:0.103119923414\n",
      "train loss:0.0713960630805\n",
      "train loss:0.162455626169\n",
      "train loss:0.243056080926\n",
      "train loss:0.0937698011793\n",
      "train loss:0.119683319496\n",
      "train loss:0.0863443700965\n",
      "train loss:0.104645513773\n",
      "train loss:0.196058616446\n",
      "train loss:0.179072945583\n",
      "train loss:0.0555411452571\n",
      "train loss:0.110692187704\n",
      "train loss:0.0833081505026\n",
      "train loss:0.21685351465\n",
      "=== epoch:2, train acc:0.966, test acc:0.966 ===\n",
      "train loss:0.084401239494\n",
      "train loss:0.0870652446037\n",
      "train loss:0.125376856743\n",
      "train loss:0.0911407401641\n",
      "train loss:0.17913434368\n",
      "train loss:0.097851861066\n",
      "train loss:0.0986103130721\n",
      "train loss:0.120999464841\n",
      "train loss:0.0707207108524\n",
      "train loss:0.0508378043768\n",
      "train loss:0.133245022657\n",
      "train loss:0.096314481664\n",
      "train loss:0.185006767984\n",
      "train loss:0.0854050802429\n",
      "train loss:0.04429839521\n",
      "train loss:0.0961719726913\n",
      "train loss:0.146138347596\n",
      "train loss:0.0510185309267\n",
      "train loss:0.0653609263392\n",
      "train loss:0.0968683236397\n",
      "train loss:0.116112640197\n",
      "train loss:0.103234615544\n",
      "train loss:0.137143324568\n",
      "train loss:0.0736475160999\n",
      "train loss:0.09084476269\n",
      "train loss:0.0667632043876\n",
      "train loss:0.0867148832429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0642272261165\n",
      "train loss:0.0658663492071\n",
      "train loss:0.0835582863268\n",
      "train loss:0.0561899897386\n",
      "train loss:0.0776791892761\n",
      "train loss:0.0938779982774\n",
      "train loss:0.136202647334\n",
      "train loss:0.0608956774945\n",
      "train loss:0.111180521486\n",
      "train loss:0.135631608664\n",
      "train loss:0.0433381814821\n",
      "train loss:0.0378916241159\n",
      "train loss:0.177992420223\n",
      "train loss:0.13115419262\n",
      "train loss:0.0331836583585\n",
      "train loss:0.068482632552\n",
      "train loss:0.184078603763\n",
      "train loss:0.195856731846\n",
      "train loss:0.109064783832\n",
      "train loss:0.151460146425\n",
      "train loss:0.0771008240149\n",
      "train loss:0.0432840427513\n",
      "train loss:0.0895077823285\n",
      "train loss:0.119295973543\n",
      "train loss:0.0554319679783\n",
      "train loss:0.107696820335\n",
      "train loss:0.090546560789\n",
      "train loss:0.0714192112339\n",
      "train loss:0.134975966321\n",
      "train loss:0.0371736702494\n",
      "train loss:0.0993704240586\n",
      "train loss:0.105112354408\n",
      "train loss:0.0684604349434\n",
      "train loss:0.0603047032183\n",
      "train loss:0.0957707388666\n",
      "train loss:0.0727634249781\n",
      "train loss:0.0818024088964\n",
      "train loss:0.0705064062786\n",
      "train loss:0.123726495631\n",
      "train loss:0.139097770724\n",
      "train loss:0.07156252645\n",
      "train loss:0.0484134713567\n",
      "train loss:0.0973901520523\n",
      "train loss:0.123074065996\n",
      "train loss:0.0849448688017\n",
      "train loss:0.0346712213667\n",
      "train loss:0.0762437323332\n",
      "train loss:0.10098842665\n",
      "train loss:0.0890674398541\n",
      "train loss:0.143609245497\n",
      "train loss:0.158545817437\n",
      "train loss:0.0648961422626\n",
      "train loss:0.0526387010023\n",
      "train loss:0.0628333619907\n",
      "train loss:0.124039463163\n",
      "train loss:0.0994389048541\n",
      "train loss:0.19084229355\n",
      "train loss:0.0974185778295\n",
      "train loss:0.0621173995322\n",
      "train loss:0.0782848956897\n",
      "train loss:0.0374109934909\n",
      "train loss:0.0442140295263\n",
      "train loss:0.0568339475911\n",
      "train loss:0.0575956427012\n",
      "train loss:0.0766149012524\n",
      "train loss:0.11791771554\n",
      "train loss:0.0699117690875\n",
      "train loss:0.149626179875\n",
      "train loss:0.0758163012986\n",
      "train loss:0.0409071715046\n",
      "train loss:0.0794078140997\n",
      "train loss:0.0705355899425\n",
      "train loss:0.133013400132\n",
      "train loss:0.0836254084034\n",
      "train loss:0.116767609077\n",
      "train loss:0.111618546882\n",
      "train loss:0.110999506562\n",
      "train loss:0.0533663958154\n",
      "train loss:0.0801365931377\n",
      "train loss:0.137176303335\n",
      "train loss:0.110621001489\n",
      "train loss:0.0756241003721\n",
      "train loss:0.0668284681498\n",
      "train loss:0.0797034197153\n",
      "train loss:0.0274804456997\n",
      "train loss:0.11738624741\n",
      "train loss:0.0528867737659\n",
      "train loss:0.0515827216163\n",
      "train loss:0.0453702027178\n",
      "train loss:0.0610013684124\n",
      "train loss:0.0639799417233\n",
      "train loss:0.0395351728375\n",
      "train loss:0.0581774328904\n",
      "train loss:0.0841189512382\n",
      "train loss:0.0576453369169\n",
      "train loss:0.106588526381\n",
      "train loss:0.10283510286\n",
      "train loss:0.0302332929476\n",
      "train loss:0.0553089040513\n",
      "train loss:0.127334513517\n",
      "train loss:0.0496749905895\n",
      "train loss:0.0884911687444\n",
      "train loss:0.143506959277\n",
      "train loss:0.130246553614\n",
      "train loss:0.105698130795\n",
      "train loss:0.118643604669\n",
      "train loss:0.125723917667\n",
      "train loss:0.0317294964862\n",
      "train loss:0.0627928824368\n",
      "train loss:0.0758741606249\n",
      "train loss:0.110580655791\n",
      "train loss:0.101422955536\n",
      "train loss:0.0466731575797\n",
      "train loss:0.0487461184747\n",
      "train loss:0.128238653842\n",
      "train loss:0.0908142129318\n",
      "train loss:0.140002295339\n",
      "train loss:0.0544692104346\n",
      "train loss:0.123576471925\n",
      "train loss:0.0580684608894\n",
      "train loss:0.0654103885575\n",
      "train loss:0.0547983595463\n",
      "train loss:0.0840466810563\n",
      "train loss:0.0462154833404\n",
      "train loss:0.0902463372055\n",
      "train loss:0.111328992522\n",
      "train loss:0.0384064138011\n",
      "train loss:0.125022702199\n",
      "train loss:0.132648425433\n",
      "train loss:0.0381802239808\n",
      "train loss:0.106971481621\n",
      "train loss:0.0984336049658\n",
      "train loss:0.0591074951411\n",
      "train loss:0.0758598799217\n",
      "train loss:0.0319312304916\n",
      "train loss:0.0930455595644\n",
      "train loss:0.0319862313042\n",
      "train loss:0.0621252837165\n",
      "train loss:0.116025922276\n",
      "train loss:0.0966573781707\n",
      "train loss:0.136534964531\n",
      "train loss:0.0773823863838\n",
      "train loss:0.107908467303\n",
      "train loss:0.0903153523472\n",
      "train loss:0.0908305613189\n",
      "train loss:0.0956844301838\n",
      "train loss:0.144185546766\n",
      "train loss:0.0613478325297\n",
      "train loss:0.064738403887\n",
      "train loss:0.0620557842478\n",
      "train loss:0.113002784872\n",
      "train loss:0.0455580119899\n",
      "train loss:0.0645127403459\n",
      "train loss:0.116908969746\n",
      "train loss:0.0566500605045\n",
      "train loss:0.139596575317\n",
      "train loss:0.0515783003865\n",
      "train loss:0.038027392854\n",
      "train loss:0.114051973165\n",
      "train loss:0.0416729042483\n",
      "train loss:0.0496587676349\n",
      "train loss:0.072869616773\n",
      "train loss:0.136140102974\n",
      "train loss:0.0671670959282\n",
      "train loss:0.124836789453\n",
      "train loss:0.106627865301\n",
      "train loss:0.103045860759\n",
      "train loss:0.0200342150939\n",
      "train loss:0.065415053321\n",
      "train loss:0.0324305574152\n",
      "train loss:0.129521188603\n",
      "train loss:0.100903990707\n",
      "train loss:0.0453562033401\n",
      "train loss:0.0364562426079\n",
      "train loss:0.0662738148633\n",
      "train loss:0.0605401601598\n",
      "train loss:0.0390011234787\n",
      "train loss:0.0500756254935\n",
      "train loss:0.0753687561094\n",
      "train loss:0.0840825796252\n",
      "train loss:0.110089622996\n",
      "train loss:0.0299349675353\n",
      "train loss:0.11394547392\n",
      "train loss:0.147669897159\n",
      "train loss:0.0809706367103\n",
      "train loss:0.105935566861\n",
      "train loss:0.0549866954827\n",
      "train loss:0.0862953213085\n",
      "train loss:0.0690737339635\n",
      "train loss:0.182003933518\n",
      "train loss:0.111405054293\n",
      "train loss:0.0394062526019\n",
      "train loss:0.0648890317945\n",
      "train loss:0.0611072607001\n",
      "train loss:0.103931155505\n",
      "train loss:0.218676265157\n",
      "train loss:0.0269340851536\n",
      "train loss:0.0627894298228\n",
      "train loss:0.0747643704779\n",
      "train loss:0.203577797345\n",
      "train loss:0.0710364466085\n",
      "train loss:0.0410300078321\n",
      "train loss:0.179098968211\n",
      "train loss:0.266727442161\n",
      "train loss:0.143946692489\n",
      "train loss:0.0467264665169\n",
      "train loss:0.0747607398454\n",
      "train loss:0.0797971049666\n",
      "train loss:0.0337295114565\n",
      "train loss:0.14361775636\n",
      "train loss:0.186174909969\n",
      "train loss:0.0586712933963\n",
      "train loss:0.0556981082116\n",
      "train loss:0.0577966164405\n",
      "train loss:0.0946274900818\n",
      "train loss:0.0968696776584\n",
      "train loss:0.125249836006\n",
      "train loss:0.0645527270273\n",
      "train loss:0.03501980516\n",
      "train loss:0.0656470903718\n",
      "train loss:0.171866255729\n",
      "train loss:0.062884251658\n",
      "train loss:0.0464358887254\n",
      "train loss:0.0948411852449\n",
      "train loss:0.0734503299206\n",
      "train loss:0.0394976960953\n",
      "train loss:0.0720479538608\n",
      "train loss:0.0321542557832\n",
      "train loss:0.0374190175845\n",
      "train loss:0.0606815554792\n",
      "train loss:0.18553157546\n",
      "train loss:0.0476389021297\n",
      "train loss:0.0333378103216\n",
      "train loss:0.0491882787113\n",
      "train loss:0.0284566735585\n",
      "train loss:0.0424948348491\n",
      "train loss:0.112076411397\n",
      "train loss:0.0426533630292\n",
      "train loss:0.0992232299153\n",
      "train loss:0.101197135136\n",
      "train loss:0.10638290761\n",
      "train loss:0.024923915164\n",
      "train loss:0.015303039332\n",
      "train loss:0.117847814983\n",
      "train loss:0.0784923916054\n",
      "train loss:0.126642041095\n",
      "train loss:0.0625267407919\n",
      "train loss:0.0245917497037\n",
      "train loss:0.0650094121984\n",
      "train loss:0.0451411069454\n",
      "train loss:0.108001696354\n",
      "train loss:0.0555449427101\n",
      "train loss:0.0407674032237\n",
      "train loss:0.122139473631\n",
      "train loss:0.0934253179812\n",
      "train loss:0.0811025612992\n",
      "train loss:0.0207529689723\n",
      "train loss:0.0879231591097\n",
      "train loss:0.0349097616166\n",
      "train loss:0.122638883454\n",
      "train loss:0.0519875722388\n",
      "train loss:0.069784916126\n",
      "train loss:0.0694342069966\n",
      "train loss:0.131706304631\n",
      "train loss:0.097604735528\n",
      "train loss:0.163603194268\n",
      "train loss:0.0700107853283\n",
      "train loss:0.124253559068\n",
      "train loss:0.0329231762873\n",
      "train loss:0.0357853524003\n",
      "train loss:0.0942284942205\n",
      "train loss:0.109627049895\n",
      "train loss:0.0334904173479\n",
      "train loss:0.106779591966\n",
      "train loss:0.0671107972201\n",
      "train loss:0.0867769214047\n",
      "train loss:0.0308218921745\n",
      "train loss:0.129824765806\n",
      "train loss:0.107929409247\n",
      "train loss:0.0306576948474\n",
      "train loss:0.0466971445241\n",
      "train loss:0.0560639006492\n",
      "train loss:0.0450892737646\n",
      "train loss:0.0637929869588\n",
      "train loss:0.0665857845049\n",
      "train loss:0.0964180976463\n",
      "train loss:0.148907398963\n",
      "train loss:0.0970925656361\n",
      "train loss:0.0323375314667\n",
      "train loss:0.0370843000141\n",
      "train loss:0.101493752078\n",
      "train loss:0.0670435284992\n",
      "train loss:0.0487455838705\n",
      "train loss:0.0727268290015\n",
      "train loss:0.0674415074061\n",
      "train loss:0.0462564829704\n",
      "train loss:0.0915736787996\n",
      "train loss:0.0510816066121\n",
      "train loss:0.0605230889556\n",
      "train loss:0.11352332635\n",
      "train loss:0.0917703073585\n",
      "train loss:0.0777892940709\n",
      "train loss:0.0523384341871\n",
      "train loss:0.0530340881671\n",
      "train loss:0.0320974031257\n",
      "train loss:0.105759106947\n",
      "train loss:0.0798727719459\n",
      "train loss:0.124554764698\n",
      "train loss:0.119174469403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0548549446636\n",
      "train loss:0.0410716221878\n",
      "train loss:0.0734490585743\n",
      "train loss:0.108126165646\n",
      "train loss:0.054153697586\n",
      "train loss:0.109167901003\n",
      "train loss:0.102067386447\n",
      "train loss:0.0653992432738\n",
      "train loss:0.0529272260444\n",
      "train loss:0.146021857716\n",
      "train loss:0.0584628906296\n",
      "train loss:0.137915014557\n",
      "train loss:0.0563688156788\n",
      "train loss:0.0268585703307\n",
      "train loss:0.0382960806962\n",
      "train loss:0.0398152599098\n",
      "train loss:0.0572790534878\n",
      "train loss:0.0941394328598\n",
      "train loss:0.0264503602162\n",
      "train loss:0.0888701167235\n",
      "train loss:0.0671687544133\n",
      "train loss:0.0962112121665\n",
      "train loss:0.0946643212992\n",
      "train loss:0.0315960793191\n",
      "train loss:0.0696253142673\n",
      "train loss:0.0520464538591\n",
      "train loss:0.0553067145907\n",
      "train loss:0.0471375680341\n",
      "train loss:0.024366084197\n",
      "train loss:0.143714428123\n",
      "train loss:0.0474405299991\n",
      "train loss:0.0430672742677\n",
      "train loss:0.0276810077962\n",
      "train loss:0.0560115813726\n",
      "train loss:0.12808406309\n",
      "train loss:0.10335570885\n",
      "train loss:0.0776982728523\n",
      "train loss:0.0772436838465\n",
      "train loss:0.0524645187891\n",
      "train loss:0.0949399177023\n",
      "train loss:0.0971792101469\n",
      "train loss:0.0607024732726\n",
      "train loss:0.0715991555232\n",
      "train loss:0.112930293184\n",
      "train loss:0.048972639922\n",
      "train loss:0.0428677488841\n",
      "train loss:0.0490497522629\n",
      "train loss:0.0690662621541\n",
      "train loss:0.0230788320578\n",
      "train loss:0.0740419709327\n",
      "train loss:0.104363169523\n",
      "train loss:0.0970664578081\n",
      "train loss:0.0792175253779\n",
      "train loss:0.0760420103382\n",
      "train loss:0.0738895259282\n",
      "train loss:0.062780673824\n",
      "train loss:0.0295920907032\n",
      "train loss:0.0207263377256\n",
      "train loss:0.0239613078012\n",
      "train loss:0.0318622186256\n",
      "train loss:0.0642384147021\n",
      "train loss:0.0894509981533\n",
      "train loss:0.0624065835412\n",
      "train loss:0.0496881250589\n",
      "train loss:0.0532994124487\n",
      "train loss:0.0599472423103\n",
      "train loss:0.0900103137462\n",
      "train loss:0.0293776490776\n",
      "train loss:0.139098727012\n",
      "train loss:0.0934554015126\n",
      "train loss:0.177061965354\n",
      "train loss:0.0464503327595\n",
      "train loss:0.107669288238\n",
      "train loss:0.0340634629769\n",
      "train loss:0.0392376746123\n",
      "train loss:0.0229170395466\n",
      "train loss:0.0902343731596\n",
      "train loss:0.200427630409\n",
      "train loss:0.0716002581279\n",
      "train loss:0.122018672339\n",
      "train loss:0.0403538573927\n",
      "train loss:0.0408627979442\n",
      "train loss:0.0237475079165\n",
      "train loss:0.0502920808308\n",
      "train loss:0.0815058048066\n",
      "train loss:0.0897575265602\n",
      "train loss:0.0670555428198\n",
      "train loss:0.0904410474656\n",
      "train loss:0.021962532495\n",
      "train loss:0.0563794153171\n",
      "train loss:0.0411152825917\n",
      "train loss:0.0360191332649\n",
      "train loss:0.0270788261344\n",
      "train loss:0.044168122358\n",
      "train loss:0.0848741277799\n",
      "train loss:0.022609094687\n",
      "train loss:0.0756509638313\n",
      "train loss:0.11599232767\n",
      "train loss:0.109687764463\n",
      "train loss:0.0386904290581\n",
      "train loss:0.111326866271\n",
      "train loss:0.111180212761\n",
      "train loss:0.102659673194\n",
      "train loss:0.105737702548\n",
      "train loss:0.069333873585\n",
      "train loss:0.0352308936511\n",
      "train loss:0.103387318719\n",
      "train loss:0.0846516340822\n",
      "train loss:0.0812433100665\n",
      "train loss:0.0478223797729\n",
      "train loss:0.0279151756125\n",
      "train loss:0.0517423325309\n",
      "train loss:0.0750223619503\n",
      "train loss:0.0322236605117\n",
      "train loss:0.0759898265884\n",
      "train loss:0.065774481327\n",
      "train loss:0.130941767156\n",
      "train loss:0.0903602931512\n",
      "train loss:0.0188901405081\n",
      "train loss:0.0365052921616\n",
      "train loss:0.0842589884612\n",
      "train loss:0.0297595700968\n",
      "train loss:0.0590881938975\n",
      "train loss:0.0902998960159\n",
      "train loss:0.0454185567717\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "#from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
