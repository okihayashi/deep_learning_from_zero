{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.12, test acc:0.0998\n",
      "epoch:1, train acc:0.143333333333, test acc:0.1093\n",
      "epoch:2, train acc:0.18, test acc:0.1277\n",
      "epoch:3, train acc:0.2, test acc:0.1344\n",
      "epoch:4, train acc:0.21, test acc:0.1426\n",
      "epoch:5, train acc:0.223333333333, test acc:0.1544\n",
      "epoch:6, train acc:0.25, test acc:0.1626\n",
      "epoch:7, train acc:0.256666666667, test acc:0.1691\n",
      "epoch:8, train acc:0.293333333333, test acc:0.1833\n",
      "epoch:9, train acc:0.306666666667, test acc:0.1918\n",
      "epoch:10, train acc:0.343333333333, test acc:0.2134\n",
      "epoch:11, train acc:0.34, test acc:0.2156\n",
      "epoch:12, train acc:0.366666666667, test acc:0.2282\n",
      "epoch:13, train acc:0.376666666667, test acc:0.2364\n",
      "epoch:14, train acc:0.39, test acc:0.2475\n",
      "epoch:15, train acc:0.39, test acc:0.259\n",
      "epoch:16, train acc:0.406666666667, test acc:0.2663\n",
      "epoch:17, train acc:0.423333333333, test acc:0.2761\n",
      "epoch:18, train acc:0.443333333333, test acc:0.289\n",
      "epoch:19, train acc:0.453333333333, test acc:0.2942\n",
      "epoch:20, train acc:0.47, test acc:0.3145\n",
      "epoch:21, train acc:0.48, test acc:0.3179\n",
      "epoch:22, train acc:0.48, test acc:0.3341\n",
      "epoch:23, train acc:0.506666666667, test acc:0.3565\n",
      "epoch:24, train acc:0.53, test acc:0.3755\n",
      "epoch:25, train acc:0.523333333333, test acc:0.3794\n",
      "epoch:26, train acc:0.53, test acc:0.3916\n",
      "epoch:27, train acc:0.543333333333, test acc:0.4113\n",
      "epoch:28, train acc:0.563333333333, test acc:0.4233\n",
      "epoch:29, train acc:0.566666666667, test acc:0.4316\n",
      "epoch:30, train acc:0.573333333333, test acc:0.4333\n",
      "epoch:31, train acc:0.573333333333, test acc:0.4416\n",
      "epoch:32, train acc:0.58, test acc:0.4474\n",
      "epoch:33, train acc:0.57, test acc:0.4457\n",
      "epoch:34, train acc:0.56, test acc:0.4453\n",
      "epoch:35, train acc:0.576666666667, test acc:0.4491\n",
      "epoch:36, train acc:0.596666666667, test acc:0.457\n",
      "epoch:37, train acc:0.59, test acc:0.4573\n",
      "epoch:38, train acc:0.6, test acc:0.4686\n",
      "epoch:39, train acc:0.6, test acc:0.4712\n",
      "epoch:40, train acc:0.593333333333, test acc:0.4738\n",
      "epoch:41, train acc:0.613333333333, test acc:0.485\n",
      "epoch:42, train acc:0.61, test acc:0.491\n",
      "epoch:43, train acc:0.603333333333, test acc:0.4929\n",
      "epoch:44, train acc:0.593333333333, test acc:0.4905\n",
      "epoch:45, train acc:0.613333333333, test acc:0.5053\n",
      "epoch:46, train acc:0.62, test acc:0.5046\n",
      "epoch:47, train acc:0.646666666667, test acc:0.5127\n",
      "epoch:48, train acc:0.633333333333, test acc:0.5135\n",
      "epoch:49, train acc:0.646666666667, test acc:0.514\n",
      "epoch:50, train acc:0.65, test acc:0.516\n",
      "epoch:51, train acc:0.646666666667, test acc:0.5156\n",
      "epoch:52, train acc:0.656666666667, test acc:0.5237\n",
      "epoch:53, train acc:0.666666666667, test acc:0.5239\n",
      "epoch:54, train acc:0.66, test acc:0.525\n",
      "epoch:55, train acc:0.68, test acc:0.5423\n",
      "epoch:56, train acc:0.703333333333, test acc:0.5451\n",
      "epoch:57, train acc:0.693333333333, test acc:0.5407\n",
      "epoch:58, train acc:0.693333333333, test acc:0.5436\n",
      "epoch:59, train acc:0.706666666667, test acc:0.5512\n",
      "epoch:60, train acc:0.746666666667, test acc:0.5686\n",
      "epoch:61, train acc:0.74, test acc:0.5629\n",
      "epoch:62, train acc:0.743333333333, test acc:0.5609\n",
      "epoch:63, train acc:0.75, test acc:0.5644\n",
      "epoch:64, train acc:0.753333333333, test acc:0.5758\n",
      "epoch:65, train acc:0.756666666667, test acc:0.5763\n",
      "epoch:66, train acc:0.783333333333, test acc:0.5879\n",
      "epoch:67, train acc:0.78, test acc:0.6019\n",
      "epoch:68, train acc:0.776666666667, test acc:0.5982\n",
      "epoch:69, train acc:0.77, test acc:0.5898\n",
      "epoch:70, train acc:0.776666666667, test acc:0.5879\n",
      "epoch:71, train acc:0.786666666667, test acc:0.5926\n",
      "epoch:72, train acc:0.793333333333, test acc:0.6116\n",
      "epoch:73, train acc:0.783333333333, test acc:0.5965\n",
      "epoch:74, train acc:0.803333333333, test acc:0.6167\n",
      "epoch:75, train acc:0.806666666667, test acc:0.6111\n",
      "epoch:76, train acc:0.826666666667, test acc:0.6224\n",
      "epoch:77, train acc:0.806666666667, test acc:0.6162\n",
      "epoch:78, train acc:0.813333333333, test acc:0.6191\n",
      "epoch:79, train acc:0.803333333333, test acc:0.6029\n",
      "epoch:80, train acc:0.806666666667, test acc:0.6024\n",
      "epoch:81, train acc:0.796666666667, test acc:0.5907\n",
      "epoch:82, train acc:0.8, test acc:0.5981\n",
      "epoch:83, train acc:0.81, test acc:0.61\n",
      "epoch:84, train acc:0.813333333333, test acc:0.6145\n",
      "epoch:85, train acc:0.816666666667, test acc:0.6234\n",
      "epoch:86, train acc:0.806666666667, test acc:0.6136\n",
      "epoch:87, train acc:0.806666666667, test acc:0.6047\n",
      "epoch:88, train acc:0.8, test acc:0.6191\n",
      "epoch:89, train acc:0.836666666667, test acc:0.6307\n",
      "epoch:90, train acc:0.823333333333, test acc:0.6208\n",
      "epoch:91, train acc:0.816666666667, test acc:0.6184\n",
      "epoch:92, train acc:0.823333333333, test acc:0.6204\n",
      "epoch:93, train acc:0.813333333333, test acc:0.6228\n",
      "epoch:94, train acc:0.83, test acc:0.6261\n",
      "epoch:95, train acc:0.846666666667, test acc:0.64\n",
      "epoch:96, train acc:0.84, test acc:0.6428\n",
      "epoch:97, train acc:0.806666666667, test acc:0.6228\n",
      "epoch:98, train acc:0.85, test acc:0.6489\n",
      "epoch:99, train acc:0.826666666667, test acc:0.6242\n",
      "epoch:100, train acc:0.826666666667, test acc:0.631\n",
      "epoch:101, train acc:0.823333333333, test acc:0.626\n",
      "epoch:102, train acc:0.85, test acc:0.6511\n",
      "epoch:103, train acc:0.846666666667, test acc:0.6448\n",
      "epoch:104, train acc:0.846666666667, test acc:0.6362\n",
      "epoch:105, train acc:0.846666666667, test acc:0.6377\n",
      "epoch:106, train acc:0.83, test acc:0.6248\n",
      "epoch:107, train acc:0.843333333333, test acc:0.6428\n",
      "epoch:108, train acc:0.843333333333, test acc:0.6443\n",
      "epoch:109, train acc:0.84, test acc:0.6317\n",
      "epoch:110, train acc:0.846666666667, test acc:0.6427\n",
      "epoch:111, train acc:0.856666666667, test acc:0.644\n",
      "epoch:112, train acc:0.86, test acc:0.6376\n",
      "epoch:113, train acc:0.856666666667, test acc:0.6564\n",
      "epoch:114, train acc:0.856666666667, test acc:0.6615\n",
      "epoch:115, train acc:0.856666666667, test acc:0.651\n",
      "epoch:116, train acc:0.85, test acc:0.6511\n",
      "epoch:117, train acc:0.86, test acc:0.653\n",
      "epoch:118, train acc:0.86, test acc:0.6521\n",
      "epoch:119, train acc:0.853333333333, test acc:0.6562\n",
      "epoch:120, train acc:0.85, test acc:0.6511\n",
      "epoch:121, train acc:0.863333333333, test acc:0.6606\n",
      "epoch:122, train acc:0.85, test acc:0.6522\n",
      "epoch:123, train acc:0.846666666667, test acc:0.6464\n",
      "epoch:124, train acc:0.843333333333, test acc:0.6459\n",
      "epoch:125, train acc:0.863333333333, test acc:0.6671\n",
      "epoch:126, train acc:0.86, test acc:0.665\n",
      "epoch:127, train acc:0.856666666667, test acc:0.6612\n",
      "epoch:128, train acc:0.856666666667, test acc:0.6587\n",
      "epoch:129, train acc:0.856666666667, test acc:0.6618\n",
      "epoch:130, train acc:0.856666666667, test acc:0.6513\n",
      "epoch:131, train acc:0.856666666667, test acc:0.6559\n",
      "epoch:132, train acc:0.85, test acc:0.6526\n",
      "epoch:133, train acc:0.863333333333, test acc:0.6546\n",
      "epoch:134, train acc:0.846666666667, test acc:0.6589\n",
      "epoch:135, train acc:0.856666666667, test acc:0.6559\n",
      "epoch:136, train acc:0.856666666667, test acc:0.6555\n",
      "epoch:137, train acc:0.843333333333, test acc:0.6504\n",
      "epoch:138, train acc:0.856666666667, test acc:0.6544\n",
      "epoch:139, train acc:0.863333333333, test acc:0.6601\n",
      "epoch:140, train acc:0.866666666667, test acc:0.6635\n",
      "epoch:141, train acc:0.866666666667, test acc:0.6631\n",
      "epoch:142, train acc:0.856666666667, test acc:0.6534\n",
      "epoch:143, train acc:0.863333333333, test acc:0.6594\n",
      "epoch:144, train acc:0.863333333333, test acc:0.6524\n",
      "epoch:145, train acc:0.863333333333, test acc:0.6479\n",
      "epoch:146, train acc:0.863333333333, test acc:0.6605\n",
      "epoch:147, train acc:0.866666666667, test acc:0.6691\n",
      "epoch:148, train acc:0.873333333333, test acc:0.6601\n",
      "epoch:149, train acc:0.883333333333, test acc:0.6593\n",
      "epoch:150, train acc:0.87, test acc:0.6598\n",
      "epoch:151, train acc:0.87, test acc:0.6628\n",
      "epoch:152, train acc:0.87, test acc:0.6658\n",
      "epoch:153, train acc:0.866666666667, test acc:0.6723\n",
      "epoch:154, train acc:0.866666666667, test acc:0.6492\n",
      "epoch:155, train acc:0.863333333333, test acc:0.6575\n",
      "epoch:156, train acc:0.863333333333, test acc:0.659\n",
      "epoch:157, train acc:0.863333333333, test acc:0.6651\n",
      "epoch:158, train acc:0.87, test acc:0.6635\n",
      "epoch:159, train acc:0.866666666667, test acc:0.6702\n",
      "epoch:160, train acc:0.866666666667, test acc:0.6631\n",
      "epoch:161, train acc:0.863333333333, test acc:0.6679\n",
      "epoch:162, train acc:0.866666666667, test acc:0.6597\n",
      "epoch:163, train acc:0.87, test acc:0.6592\n",
      "epoch:164, train acc:0.866666666667, test acc:0.6618\n",
      "epoch:165, train acc:0.873333333333, test acc:0.6678\n",
      "epoch:166, train acc:0.866666666667, test acc:0.6609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:167, train acc:0.86, test acc:0.659\n",
      "epoch:168, train acc:0.87, test acc:0.6603\n",
      "epoch:169, train acc:0.873333333333, test acc:0.6611\n",
      "epoch:170, train acc:0.866666666667, test acc:0.6607\n",
      "epoch:171, train acc:0.87, test acc:0.6604\n",
      "epoch:172, train acc:0.863333333333, test acc:0.6731\n",
      "epoch:173, train acc:0.853333333333, test acc:0.6718\n",
      "epoch:174, train acc:0.873333333333, test acc:0.6655\n",
      "epoch:175, train acc:0.866666666667, test acc:0.6693\n",
      "epoch:176, train acc:0.873333333333, test acc:0.6625\n",
      "epoch:177, train acc:0.87, test acc:0.6609\n",
      "epoch:178, train acc:0.87, test acc:0.661\n",
      "epoch:179, train acc:0.88, test acc:0.6727\n",
      "epoch:180, train acc:0.86, test acc:0.662\n",
      "epoch:181, train acc:0.883333333333, test acc:0.6656\n",
      "epoch:182, train acc:0.88, test acc:0.6674\n",
      "epoch:183, train acc:0.876666666667, test acc:0.6673\n",
      "epoch:184, train acc:0.876666666667, test acc:0.6646\n",
      "epoch:185, train acc:0.87, test acc:0.6515\n",
      "epoch:186, train acc:0.866666666667, test acc:0.6606\n",
      "epoch:187, train acc:0.873333333333, test acc:0.6649\n",
      "epoch:188, train acc:0.876666666667, test acc:0.6557\n",
      "epoch:189, train acc:0.856666666667, test acc:0.6553\n",
      "epoch:190, train acc:0.85, test acc:0.6596\n",
      "epoch:191, train acc:0.866666666667, test acc:0.6639\n",
      "epoch:192, train acc:0.873333333333, test acc:0.6532\n",
      "epoch:193, train acc:0.883333333333, test acc:0.6614\n",
      "epoch:194, train acc:0.873333333333, test acc:0.6593\n",
      "epoch:195, train acc:0.876666666667, test acc:0.6578\n",
      "epoch:196, train acc:0.876666666667, test acc:0.6686\n",
      "epoch:197, train acc:0.873333333333, test acc:0.6658\n",
      "epoch:198, train acc:0.866666666667, test acc:0.6631\n",
      "epoch:199, train acc:0.873333333333, test acc:0.6686\n",
      "epoch:200, train acc:0.876666666667, test acc:0.6598\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dad2588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 過学習を再現するために、学習データを削減\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# weight decay（荷重減衰）の設定 =======================\n",
    "#weight_decay_lambda = 0 # weight decayを使用しない場合\n",
    "weight_decay_lambda = 0.1\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01)\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break\n",
    "\n",
    "\n",
    "# 3.グラフの描画==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.0966666666667, test acc:0.0902\n",
      "epoch:1, train acc:0.09, test acc:0.0925\n",
      "epoch:2, train acc:0.0966666666667, test acc:0.1021\n",
      "epoch:3, train acc:0.12, test acc:0.1144\n",
      "epoch:4, train acc:0.14, test acc:0.1298\n",
      "epoch:5, train acc:0.183333333333, test acc:0.1517\n",
      "epoch:6, train acc:0.206666666667, test acc:0.1768\n",
      "epoch:7, train acc:0.236666666667, test acc:0.2012\n",
      "epoch:8, train acc:0.26, test acc:0.2188\n",
      "epoch:9, train acc:0.286666666667, test acc:0.2351\n",
      "epoch:10, train acc:0.293333333333, test acc:0.2516\n",
      "epoch:11, train acc:0.323333333333, test acc:0.2617\n",
      "epoch:12, train acc:0.333333333333, test acc:0.2723\n",
      "epoch:13, train acc:0.356666666667, test acc:0.2861\n",
      "epoch:14, train acc:0.386666666667, test acc:0.2998\n",
      "epoch:15, train acc:0.396666666667, test acc:0.3097\n",
      "epoch:16, train acc:0.42, test acc:0.3189\n",
      "epoch:17, train acc:0.42, test acc:0.3311\n",
      "epoch:18, train acc:0.433333333333, test acc:0.3399\n",
      "epoch:19, train acc:0.45, test acc:0.3543\n",
      "epoch:20, train acc:0.463333333333, test acc:0.3592\n",
      "epoch:21, train acc:0.456666666667, test acc:0.3638\n",
      "epoch:22, train acc:0.47, test acc:0.372\n",
      "epoch:23, train acc:0.48, test acc:0.3784\n",
      "epoch:24, train acc:0.496666666667, test acc:0.3873\n",
      "epoch:25, train acc:0.496666666667, test acc:0.3977\n",
      "epoch:26, train acc:0.51, test acc:0.4024\n",
      "epoch:27, train acc:0.51, test acc:0.4029\n",
      "epoch:28, train acc:0.513333333333, test acc:0.402\n",
      "epoch:29, train acc:0.503333333333, test acc:0.4019\n",
      "epoch:30, train acc:0.5, test acc:0.4036\n",
      "epoch:31, train acc:0.506666666667, test acc:0.407\n",
      "epoch:32, train acc:0.51, test acc:0.4067\n",
      "epoch:33, train acc:0.516666666667, test acc:0.4118\n",
      "epoch:34, train acc:0.506666666667, test acc:0.4091\n",
      "epoch:35, train acc:0.506666666667, test acc:0.4117\n",
      "epoch:36, train acc:0.51, test acc:0.4161\n",
      "epoch:37, train acc:0.536666666667, test acc:0.4262\n",
      "epoch:38, train acc:0.55, test acc:0.4308\n",
      "epoch:39, train acc:0.536666666667, test acc:0.4331\n",
      "epoch:40, train acc:0.573333333333, test acc:0.4496\n",
      "epoch:41, train acc:0.576666666667, test acc:0.4643\n",
      "epoch:42, train acc:0.583333333333, test acc:0.4611\n",
      "epoch:43, train acc:0.596666666667, test acc:0.4782\n",
      "epoch:44, train acc:0.603333333333, test acc:0.4798\n",
      "epoch:45, train acc:0.613333333333, test acc:0.4866\n",
      "epoch:46, train acc:0.626666666667, test acc:0.4843\n",
      "epoch:47, train acc:0.623333333333, test acc:0.5097\n",
      "epoch:48, train acc:0.646666666667, test acc:0.5027\n",
      "epoch:49, train acc:0.65, test acc:0.5199\n",
      "epoch:50, train acc:0.656666666667, test acc:0.5249\n",
      "epoch:51, train acc:0.646666666667, test acc:0.5234\n",
      "epoch:52, train acc:0.683333333333, test acc:0.5445\n",
      "epoch:53, train acc:0.67, test acc:0.547\n",
      "epoch:54, train acc:0.656666666667, test acc:0.5291\n",
      "epoch:55, train acc:0.68, test acc:0.5478\n",
      "epoch:56, train acc:0.686666666667, test acc:0.5561\n",
      "epoch:57, train acc:0.703333333333, test acc:0.5623\n",
      "epoch:58, train acc:0.693333333333, test acc:0.5582\n",
      "epoch:59, train acc:0.716666666667, test acc:0.5595\n",
      "epoch:60, train acc:0.726666666667, test acc:0.5696\n",
      "epoch:61, train acc:0.726666666667, test acc:0.5723\n",
      "epoch:62, train acc:0.74, test acc:0.5924\n",
      "epoch:63, train acc:0.753333333333, test acc:0.5992\n",
      "epoch:64, train acc:0.753333333333, test acc:0.6012\n",
      "epoch:65, train acc:0.75, test acc:0.6009\n",
      "epoch:66, train acc:0.753333333333, test acc:0.6172\n",
      "epoch:67, train acc:0.756666666667, test acc:0.6176\n",
      "epoch:68, train acc:0.763333333333, test acc:0.6107\n",
      "epoch:69, train acc:0.766666666667, test acc:0.6203\n",
      "epoch:70, train acc:0.763333333333, test acc:0.6332\n",
      "epoch:71, train acc:0.77, test acc:0.618\n",
      "epoch:72, train acc:0.773333333333, test acc:0.6202\n",
      "epoch:73, train acc:0.773333333333, test acc:0.6281\n",
      "epoch:74, train acc:0.766666666667, test acc:0.6225\n",
      "epoch:75, train acc:0.77, test acc:0.6313\n",
      "epoch:76, train acc:0.773333333333, test acc:0.6358\n",
      "epoch:77, train acc:0.77, test acc:0.6343\n",
      "epoch:78, train acc:0.78, test acc:0.6443\n",
      "epoch:79, train acc:0.77, test acc:0.633\n",
      "epoch:80, train acc:0.776666666667, test acc:0.6418\n",
      "epoch:81, train acc:0.78, test acc:0.6393\n",
      "epoch:82, train acc:0.783333333333, test acc:0.6342\n",
      "epoch:83, train acc:0.806666666667, test acc:0.6526\n",
      "epoch:84, train acc:0.79, test acc:0.6511\n",
      "epoch:85, train acc:0.806666666667, test acc:0.6593\n",
      "epoch:86, train acc:0.803333333333, test acc:0.6501\n",
      "epoch:87, train acc:0.81, test acc:0.6576\n",
      "epoch:88, train acc:0.813333333333, test acc:0.671\n",
      "epoch:89, train acc:0.8, test acc:0.6647\n",
      "epoch:90, train acc:0.82, test acc:0.6692\n",
      "epoch:91, train acc:0.81, test acc:0.6601\n",
      "epoch:92, train acc:0.813333333333, test acc:0.6648\n",
      "epoch:93, train acc:0.813333333333, test acc:0.657\n",
      "epoch:94, train acc:0.793333333333, test acc:0.6564\n",
      "epoch:95, train acc:0.816666666667, test acc:0.6644\n",
      "epoch:96, train acc:0.823333333333, test acc:0.6741\n",
      "epoch:97, train acc:0.806666666667, test acc:0.6577\n",
      "epoch:98, train acc:0.82, test acc:0.6684\n",
      "epoch:99, train acc:0.806666666667, test acc:0.6603\n",
      "epoch:100, train acc:0.81, test acc:0.671\n",
      "epoch:101, train acc:0.803333333333, test acc:0.657\n",
      "epoch:102, train acc:0.816666666667, test acc:0.6672\n",
      "epoch:103, train acc:0.81, test acc:0.674\n",
      "epoch:104, train acc:0.823333333333, test acc:0.687\n",
      "epoch:105, train acc:0.816666666667, test acc:0.6779\n",
      "epoch:106, train acc:0.82, test acc:0.682\n",
      "epoch:107, train acc:0.836666666667, test acc:0.6853\n",
      "epoch:108, train acc:0.826666666667, test acc:0.6836\n",
      "epoch:109, train acc:0.823333333333, test acc:0.6895\n",
      "epoch:110, train acc:0.823333333333, test acc:0.6853\n",
      "epoch:111, train acc:0.82, test acc:0.6859\n",
      "epoch:112, train acc:0.826666666667, test acc:0.6868\n",
      "epoch:113, train acc:0.82, test acc:0.6875\n",
      "epoch:114, train acc:0.816666666667, test acc:0.6875\n",
      "epoch:115, train acc:0.826666666667, test acc:0.6834\n",
      "epoch:116, train acc:0.826666666667, test acc:0.6913\n",
      "epoch:117, train acc:0.826666666667, test acc:0.6861\n",
      "epoch:118, train acc:0.833333333333, test acc:0.6915\n",
      "epoch:119, train acc:0.82, test acc:0.6901\n",
      "epoch:120, train acc:0.843333333333, test acc:0.6969\n",
      "epoch:121, train acc:0.833333333333, test acc:0.6929\n",
      "epoch:122, train acc:0.84, test acc:0.6978\n",
      "epoch:123, train acc:0.826666666667, test acc:0.6892\n",
      "epoch:124, train acc:0.83, test acc:0.6917\n",
      "epoch:125, train acc:0.843333333333, test acc:0.6985\n",
      "epoch:126, train acc:0.833333333333, test acc:0.6912\n",
      "epoch:127, train acc:0.846666666667, test acc:0.6893\n",
      "epoch:128, train acc:0.826666666667, test acc:0.6873\n",
      "epoch:129, train acc:0.823333333333, test acc:0.6848\n",
      "epoch:130, train acc:0.843333333333, test acc:0.7043\n",
      "epoch:131, train acc:0.833333333333, test acc:0.6926\n",
      "epoch:132, train acc:0.83, test acc:0.6967\n",
      "epoch:133, train acc:0.856666666667, test acc:0.7064\n",
      "epoch:134, train acc:0.846666666667, test acc:0.7046\n",
      "epoch:135, train acc:0.836666666667, test acc:0.6981\n",
      "epoch:136, train acc:0.843333333333, test acc:0.7012\n",
      "epoch:137, train acc:0.85, test acc:0.7033\n",
      "epoch:138, train acc:0.84, test acc:0.692\n",
      "epoch:139, train acc:0.836666666667, test acc:0.6923\n",
      "epoch:140, train acc:0.836666666667, test acc:0.6873\n",
      "epoch:141, train acc:0.84, test acc:0.6934\n",
      "epoch:142, train acc:0.846666666667, test acc:0.6943\n",
      "epoch:143, train acc:0.856666666667, test acc:0.6999\n",
      "epoch:144, train acc:0.84, test acc:0.704\n",
      "epoch:145, train acc:0.856666666667, test acc:0.7051\n",
      "epoch:146, train acc:0.833333333333, test acc:0.6952\n",
      "epoch:147, train acc:0.853333333333, test acc:0.7018\n",
      "epoch:148, train acc:0.856666666667, test acc:0.7034\n",
      "epoch:149, train acc:0.856666666667, test acc:0.7098\n",
      "epoch:150, train acc:0.846666666667, test acc:0.6999\n",
      "epoch:151, train acc:0.856666666667, test acc:0.6911\n",
      "epoch:152, train acc:0.853333333333, test acc:0.7053\n",
      "epoch:153, train acc:0.846666666667, test acc:0.6998\n",
      "epoch:154, train acc:0.846666666667, test acc:0.7049\n",
      "epoch:155, train acc:0.84, test acc:0.6969\n",
      "epoch:156, train acc:0.846666666667, test acc:0.6986\n",
      "epoch:157, train acc:0.846666666667, test acc:0.6969\n",
      "epoch:158, train acc:0.843333333333, test acc:0.6984\n",
      "epoch:159, train acc:0.846666666667, test acc:0.7013\n",
      "epoch:160, train acc:0.85, test acc:0.7073\n",
      "epoch:161, train acc:0.843333333333, test acc:0.7033\n",
      "epoch:162, train acc:0.85, test acc:0.7023\n",
      "epoch:163, train acc:0.85, test acc:0.7062\n",
      "epoch:164, train acc:0.85, test acc:0.6926\n",
      "epoch:165, train acc:0.846666666667, test acc:0.7045\n",
      "epoch:166, train acc:0.86, test acc:0.7131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:167, train acc:0.85, test acc:0.7084\n",
      "epoch:168, train acc:0.85, test acc:0.699\n",
      "epoch:169, train acc:0.853333333333, test acc:0.7032\n",
      "epoch:170, train acc:0.853333333333, test acc:0.7051\n",
      "epoch:171, train acc:0.846666666667, test acc:0.7045\n",
      "epoch:172, train acc:0.846666666667, test acc:0.7069\n",
      "epoch:173, train acc:0.843333333333, test acc:0.7049\n",
      "epoch:174, train acc:0.86, test acc:0.7046\n",
      "epoch:175, train acc:0.853333333333, test acc:0.7116\n",
      "epoch:176, train acc:0.85, test acc:0.7092\n",
      "epoch:177, train acc:0.843333333333, test acc:0.702\n",
      "epoch:178, train acc:0.843333333333, test acc:0.6942\n",
      "epoch:179, train acc:0.846666666667, test acc:0.6977\n",
      "epoch:180, train acc:0.853333333333, test acc:0.7015\n",
      "epoch:181, train acc:0.853333333333, test acc:0.699\n",
      "epoch:182, train acc:0.85, test acc:0.7031\n",
      "epoch:183, train acc:0.853333333333, test acc:0.7096\n",
      "epoch:184, train acc:0.846666666667, test acc:0.7057\n",
      "epoch:185, train acc:0.85, test acc:0.7096\n",
      "epoch:186, train acc:0.85, test acc:0.7012\n",
      "epoch:187, train acc:0.85, test acc:0.7049\n",
      "epoch:188, train acc:0.863333333333, test acc:0.7089\n",
      "epoch:189, train acc:0.866666666667, test acc:0.7079\n",
      "epoch:190, train acc:0.863333333333, test acc:0.7026\n",
      "epoch:191, train acc:0.86, test acc:0.7038\n",
      "epoch:192, train acc:0.856666666667, test acc:0.704\n",
      "epoch:193, train acc:0.863333333333, test acc:0.7122\n",
      "epoch:194, train acc:0.86, test acc:0.7045\n",
      "epoch:195, train acc:0.863333333333, test acc:0.7129\n",
      "epoch:196, train acc:0.86, test acc:0.7058\n",
      "epoch:197, train acc:0.85, test acc:0.7062\n",
      "epoch:198, train acc:0.85, test acc:0.7032\n",
      "epoch:199, train acc:0.853333333333, test acc:0.7024\n",
      "epoch:200, train acc:0.85, test acc:0.7067\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c59fa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 過学習を再現するために、学習データを削減\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# weight decay（荷重減衰）の設定 =======================\n",
    "#weight_decay_lambda = 0 # weight decayを使用しない場合\n",
    "weight_decay_lambda = 0.1\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01)\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break\n",
    "\n",
    "\n",
    "# 3.グラフの描画==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.30265020076\n",
      "=== epoch:1, train acc:0.0633333333333, test acc:0.0844 ===\n",
      "train loss:2.30974034664\n",
      "train loss:2.30871770064\n",
      "train loss:2.31132613862\n",
      "=== epoch:2, train acc:0.0633333333333, test acc:0.0836 ===\n",
      "train loss:2.31363457446\n",
      "train loss:2.30081147324\n",
      "train loss:2.30672381903\n",
      "=== epoch:3, train acc:0.0633333333333, test acc:0.0839 ===\n",
      "train loss:2.31110683574\n",
      "train loss:2.30542026425\n",
      "train loss:2.30678888165\n",
      "=== epoch:4, train acc:0.06, test acc:0.0852 ===\n",
      "train loss:2.31091670847\n",
      "train loss:2.30468174922\n",
      "train loss:2.31272601228\n",
      "=== epoch:5, train acc:0.06, test acc:0.0857 ===\n",
      "train loss:2.30574991057\n",
      "train loss:2.3072468793\n",
      "train loss:2.30667352066\n",
      "=== epoch:6, train acc:0.0633333333333, test acc:0.0887 ===\n",
      "train loss:2.30184989171\n",
      "train loss:2.29785941215\n",
      "train loss:2.30304051983\n",
      "=== epoch:7, train acc:0.0533333333333, test acc:0.0908 ===\n",
      "train loss:2.31097439436\n",
      "train loss:2.30714709986\n",
      "train loss:2.30582607465\n",
      "=== epoch:8, train acc:0.0566666666667, test acc:0.0935 ===\n",
      "train loss:2.30092853825\n",
      "train loss:2.30188612045\n",
      "train loss:2.30038642908\n",
      "=== epoch:9, train acc:0.06, test acc:0.0949 ===\n",
      "train loss:2.30728634233\n",
      "train loss:2.30441119947\n",
      "train loss:2.30105642069\n",
      "=== epoch:10, train acc:0.0633333333333, test acc:0.0972 ===\n",
      "train loss:2.30009925279\n",
      "train loss:2.2950474006\n",
      "train loss:2.29929057048\n",
      "=== epoch:11, train acc:0.0666666666667, test acc:0.1018 ===\n",
      "train loss:2.29741739651\n",
      "train loss:2.29533714881\n",
      "train loss:2.2889611127\n",
      "=== epoch:12, train acc:0.08, test acc:0.1083 ===\n",
      "train loss:2.29940117422\n",
      "train loss:2.29460693026\n",
      "train loss:2.29963150531\n",
      "=== epoch:13, train acc:0.09, test acc:0.1118 ===\n",
      "train loss:2.29041520826\n",
      "train loss:2.29342684595\n",
      "train loss:2.2960978093\n",
      "=== epoch:14, train acc:0.106666666667, test acc:0.1155 ===\n",
      "train loss:2.28538717595\n",
      "train loss:2.28661376351\n",
      "train loss:2.29270995758\n",
      "=== epoch:15, train acc:0.106666666667, test acc:0.1184 ===\n",
      "train loss:2.29991917407\n",
      "train loss:2.29818998982\n",
      "train loss:2.29269783172\n",
      "=== epoch:16, train acc:0.113333333333, test acc:0.1195 ===\n",
      "train loss:2.28683345001\n",
      "train loss:2.28790638533\n",
      "train loss:2.2903005896\n",
      "=== epoch:17, train acc:0.12, test acc:0.1217 ===\n",
      "train loss:2.29288345421\n",
      "train loss:2.30144254\n",
      "train loss:2.29052205554\n",
      "=== epoch:18, train acc:0.12, test acc:0.1244 ===\n",
      "train loss:2.28479740777\n",
      "train loss:2.29151986935\n",
      "train loss:2.28374568701\n",
      "=== epoch:19, train acc:0.126666666667, test acc:0.1277 ===\n",
      "train loss:2.29026484515\n",
      "train loss:2.29195468826\n",
      "train loss:2.29259978698\n",
      "=== epoch:20, train acc:0.133333333333, test acc:0.1325 ===\n",
      "train loss:2.29427222229\n",
      "train loss:2.28189715431\n",
      "train loss:2.28505066832\n",
      "=== epoch:21, train acc:0.146666666667, test acc:0.1394 ===\n",
      "train loss:2.29405913028\n",
      "train loss:2.28500984993\n",
      "train loss:2.27952852039\n",
      "=== epoch:22, train acc:0.146666666667, test acc:0.1431 ===\n",
      "train loss:2.27932987304\n",
      "train loss:2.30264149613\n",
      "train loss:2.2812513688\n",
      "=== epoch:23, train acc:0.146666666667, test acc:0.1449 ===\n",
      "train loss:2.29344738143\n",
      "train loss:2.29270459262\n",
      "train loss:2.28068455764\n",
      "=== epoch:24, train acc:0.156666666667, test acc:0.1495 ===\n",
      "train loss:2.27480276722\n",
      "train loss:2.30192808412\n",
      "train loss:2.28970438538\n",
      "=== epoch:25, train acc:0.17, test acc:0.1522 ===\n",
      "train loss:2.28493295498\n",
      "train loss:2.28673294458\n",
      "train loss:2.28526519085\n",
      "=== epoch:26, train acc:0.176666666667, test acc:0.1562 ===\n",
      "train loss:2.27515303549\n",
      "train loss:2.28214021857\n",
      "train loss:2.28087355833\n",
      "=== epoch:27, train acc:0.18, test acc:0.1614 ===\n",
      "train loss:2.26970142037\n",
      "train loss:2.27574641304\n",
      "train loss:2.28312474168\n",
      "=== epoch:28, train acc:0.19, test acc:0.1669 ===\n",
      "train loss:2.27233490345\n",
      "train loss:2.27935983118\n",
      "train loss:2.28761024413\n",
      "=== epoch:29, train acc:0.2, test acc:0.1717 ===\n",
      "train loss:2.28870286917\n",
      "train loss:2.28110083962\n",
      "train loss:2.27720871881\n",
      "=== epoch:30, train acc:0.203333333333, test acc:0.1759 ===\n",
      "train loss:2.28088021071\n",
      "train loss:2.27407512127\n",
      "train loss:2.2783866534\n",
      "=== epoch:31, train acc:0.206666666667, test acc:0.1798 ===\n",
      "train loss:2.27293925995\n",
      "train loss:2.2830057192\n",
      "train loss:2.28121167814\n",
      "=== epoch:32, train acc:0.203333333333, test acc:0.1804 ===\n",
      "train loss:2.28245657134\n",
      "train loss:2.2922165696\n",
      "train loss:2.27829146288\n",
      "=== epoch:33, train acc:0.206666666667, test acc:0.185 ===\n",
      "train loss:2.26332916534\n",
      "train loss:2.2839703795\n",
      "train loss:2.2746638482\n",
      "=== epoch:34, train acc:0.21, test acc:0.1903 ===\n",
      "train loss:2.27683049885\n",
      "train loss:2.26594751965\n",
      "train loss:2.27163613698\n",
      "=== epoch:35, train acc:0.216666666667, test acc:0.1948 ===\n",
      "train loss:2.26487327851\n",
      "train loss:2.27651036888\n",
      "train loss:2.27837694248\n",
      "=== epoch:36, train acc:0.23, test acc:0.1954 ===\n",
      "train loss:2.27240005376\n",
      "train loss:2.27184948351\n",
      "train loss:2.26459944016\n",
      "=== epoch:37, train acc:0.226666666667, test acc:0.1982 ===\n",
      "train loss:2.26837413234\n",
      "train loss:2.27316636856\n",
      "train loss:2.27386668442\n",
      "=== epoch:38, train acc:0.236666666667, test acc:0.2034 ===\n",
      "train loss:2.26775538451\n",
      "train loss:2.26929141036\n",
      "train loss:2.27381662036\n",
      "=== epoch:39, train acc:0.24, test acc:0.2028 ===\n",
      "train loss:2.26936850961\n",
      "train loss:2.27826729123\n",
      "train loss:2.25973352772\n",
      "=== epoch:40, train acc:0.233333333333, test acc:0.205 ===\n",
      "train loss:2.26565525625\n",
      "train loss:2.27140956422\n",
      "train loss:2.26272893524\n",
      "=== epoch:41, train acc:0.236666666667, test acc:0.2042 ===\n",
      "train loss:2.26549352265\n",
      "train loss:2.27320286659\n",
      "train loss:2.27688914912\n",
      "=== epoch:42, train acc:0.233333333333, test acc:0.2034 ===\n",
      "train loss:2.27082102694\n",
      "train loss:2.27539328401\n",
      "train loss:2.27082055758\n",
      "=== epoch:43, train acc:0.233333333333, test acc:0.2058 ===\n",
      "train loss:2.26459224955\n",
      "train loss:2.27303525846\n",
      "train loss:2.26761120444\n",
      "=== epoch:44, train acc:0.23, test acc:0.208 ===\n",
      "train loss:2.24647397379\n",
      "train loss:2.27816570575\n",
      "train loss:2.25700081791\n",
      "=== epoch:45, train acc:0.233333333333, test acc:0.2095 ===\n",
      "train loss:2.2724696033\n",
      "train loss:2.24816746871\n",
      "train loss:2.25967826013\n",
      "=== epoch:46, train acc:0.236666666667, test acc:0.2128 ===\n",
      "train loss:2.27687383104\n",
      "train loss:2.26577767734\n",
      "train loss:2.26541157869\n",
      "=== epoch:47, train acc:0.236666666667, test acc:0.2141 ===\n",
      "train loss:2.27223445023\n",
      "train loss:2.26624933396\n",
      "train loss:2.27275637021\n",
      "=== epoch:48, train acc:0.25, test acc:0.2159 ===\n",
      "train loss:2.24303758988\n",
      "train loss:2.27434652646\n",
      "train loss:2.25864736596\n",
      "=== epoch:49, train acc:0.25, test acc:0.2178 ===\n",
      "train loss:2.24979883268\n",
      "train loss:2.25662764379\n",
      "train loss:2.26753118562\n",
      "=== epoch:50, train acc:0.246666666667, test acc:0.2203 ===\n",
      "train loss:2.24306036772\n",
      "train loss:2.25920180402\n",
      "train loss:2.24876116008\n",
      "=== epoch:51, train acc:0.243333333333, test acc:0.216 ===\n",
      "train loss:2.26923819564\n",
      "train loss:2.25200226219\n",
      "train loss:2.27033682735\n",
      "=== epoch:52, train acc:0.24, test acc:0.2172 ===\n",
      "train loss:2.23736420903\n",
      "train loss:2.28480200118\n",
      "train loss:2.23707975296\n",
      "=== epoch:53, train acc:0.25, test acc:0.2204 ===\n",
      "train loss:2.24837537946\n",
      "train loss:2.26981639725\n",
      "train loss:2.25183985205\n",
      "=== epoch:54, train acc:0.25, test acc:0.2193 ===\n",
      "train loss:2.26243034441\n",
      "train loss:2.23777739277\n",
      "train loss:2.27247742068\n",
      "=== epoch:55, train acc:0.246666666667, test acc:0.2195 ===\n",
      "train loss:2.23062882474\n",
      "train loss:2.23158464312\n",
      "train loss:2.26084809072\n",
      "=== epoch:56, train acc:0.243333333333, test acc:0.2204 ===\n",
      "train loss:2.23812938225\n",
      "train loss:2.2427913576\n",
      "train loss:2.24887411139\n",
      "=== epoch:57, train acc:0.25, test acc:0.2223 ===\n",
      "train loss:2.23294388304\n",
      "train loss:2.26286845768\n",
      "train loss:2.23550717508\n",
      "=== epoch:58, train acc:0.236666666667, test acc:0.2224 ===\n",
      "train loss:2.24678244614\n",
      "train loss:2.24640714234\n",
      "train loss:2.24345637452\n",
      "=== epoch:59, train acc:0.243333333333, test acc:0.223 ===\n",
      "train loss:2.24000181555\n",
      "train loss:2.23999450739\n",
      "train loss:2.24503186591\n",
      "=== epoch:60, train acc:0.243333333333, test acc:0.2237 ===\n",
      "train loss:2.22275047655\n",
      "train loss:2.24722300036\n",
      "train loss:2.23937500708\n",
      "=== epoch:61, train acc:0.246666666667, test acc:0.2254 ===\n",
      "train loss:2.22861331034\n",
      "train loss:2.23103017094\n",
      "train loss:2.24796070854\n",
      "=== epoch:62, train acc:0.25, test acc:0.2265 ===\n",
      "train loss:2.23617657811\n",
      "train loss:2.23273684188\n",
      "train loss:2.26268557865\n",
      "=== epoch:63, train acc:0.25, test acc:0.2259 ===\n",
      "train loss:2.22256133022\n",
      "train loss:2.24529628119\n",
      "train loss:2.22199841284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:64, train acc:0.25, test acc:0.2268 ===\n",
      "train loss:2.22288646512\n",
      "train loss:2.23395700749\n",
      "train loss:2.21799178239\n",
      "=== epoch:65, train acc:0.253333333333, test acc:0.2264 ===\n",
      "train loss:2.24435166214\n",
      "train loss:2.22881063452\n",
      "train loss:2.24771502373\n",
      "=== epoch:66, train acc:0.26, test acc:0.2315 ===\n",
      "train loss:2.22357099914\n",
      "train loss:2.22943612291\n",
      "train loss:2.23814052551\n",
      "=== epoch:67, train acc:0.26, test acc:0.2335 ===\n",
      "train loss:2.18806443696\n",
      "train loss:2.18938480843\n",
      "train loss:2.23088961314\n",
      "=== epoch:68, train acc:0.253333333333, test acc:0.2327 ===\n",
      "train loss:2.23536497235\n",
      "train loss:2.21234946688\n",
      "train loss:2.24029111122\n",
      "=== epoch:69, train acc:0.256666666667, test acc:0.2326 ===\n",
      "train loss:2.22823793939\n",
      "train loss:2.25431637274\n",
      "train loss:2.22340861975\n",
      "=== epoch:70, train acc:0.26, test acc:0.2334 ===\n",
      "train loss:2.23402002596\n",
      "train loss:2.21831161879\n",
      "train loss:2.23120233035\n",
      "=== epoch:71, train acc:0.256666666667, test acc:0.2329 ===\n",
      "train loss:2.22903994458\n",
      "train loss:2.25501399752\n",
      "train loss:2.22101919245\n",
      "=== epoch:72, train acc:0.26, test acc:0.2342 ===\n",
      "train loss:2.19091965173\n",
      "train loss:2.19721630309\n",
      "train loss:2.2243921016\n",
      "=== epoch:73, train acc:0.25, test acc:0.2309 ===\n",
      "train loss:2.21649567465\n",
      "train loss:2.21282099309\n",
      "train loss:2.23153354641\n",
      "=== epoch:74, train acc:0.25, test acc:0.2302 ===\n",
      "train loss:2.2198187474\n",
      "train loss:2.20500154522\n",
      "train loss:2.21117448353\n",
      "=== epoch:75, train acc:0.25, test acc:0.2303 ===\n",
      "train loss:2.21526762985\n",
      "train loss:2.17044116563\n",
      "train loss:2.20097296104\n",
      "=== epoch:76, train acc:0.25, test acc:0.23 ===\n",
      "train loss:2.19664264978\n",
      "train loss:2.18771132443\n",
      "train loss:2.23961558591\n",
      "=== epoch:77, train acc:0.246666666667, test acc:0.2294 ===\n",
      "train loss:2.17531573028\n",
      "train loss:2.19721586225\n",
      "train loss:2.21052535109\n",
      "=== epoch:78, train acc:0.246666666667, test acc:0.2282 ===\n",
      "train loss:2.18291072371\n",
      "train loss:2.22218680486\n",
      "train loss:2.22834631178\n",
      "=== epoch:79, train acc:0.253333333333, test acc:0.2284 ===\n",
      "train loss:2.21966960333\n",
      "train loss:2.18708856199\n",
      "train loss:2.14881248857\n",
      "=== epoch:80, train acc:0.246666666667, test acc:0.2268 ===\n",
      "train loss:2.17453138666\n",
      "train loss:2.23684102891\n",
      "train loss:2.199519816\n",
      "=== epoch:81, train acc:0.253333333333, test acc:0.2271 ===\n",
      "train loss:2.1801649005\n",
      "train loss:2.17919057698\n",
      "train loss:2.22173142797\n",
      "=== epoch:82, train acc:0.246666666667, test acc:0.2275 ===\n",
      "train loss:2.20974467002\n",
      "train loss:2.20358017113\n",
      "train loss:2.20164420384\n",
      "=== epoch:83, train acc:0.243333333333, test acc:0.2265 ===\n",
      "train loss:2.17959314985\n",
      "train loss:2.18182918444\n",
      "train loss:2.18879814069\n",
      "=== epoch:84, train acc:0.243333333333, test acc:0.225 ===\n",
      "train loss:2.19577869967\n",
      "train loss:2.17368218834\n",
      "train loss:2.22240422013\n",
      "=== epoch:85, train acc:0.246666666667, test acc:0.2247 ===\n",
      "train loss:2.23060438635\n",
      "train loss:2.15131732326\n",
      "train loss:2.19713126716\n",
      "=== epoch:86, train acc:0.243333333333, test acc:0.224 ===\n",
      "train loss:2.18112399817\n",
      "train loss:2.19286835809\n",
      "train loss:2.14051021827\n",
      "=== epoch:87, train acc:0.246666666667, test acc:0.2237 ===\n",
      "train loss:2.17847464589\n",
      "train loss:2.18997150528\n",
      "train loss:2.17425846512\n",
      "=== epoch:88, train acc:0.243333333333, test acc:0.2227 ===\n",
      "train loss:2.18405828281\n",
      "train loss:2.17239538794\n",
      "train loss:2.1673494539\n",
      "=== epoch:89, train acc:0.25, test acc:0.223 ===\n",
      "train loss:2.19294745409\n",
      "train loss:2.16347227716\n",
      "train loss:2.19942025918\n",
      "=== epoch:90, train acc:0.253333333333, test acc:0.2261 ===\n",
      "train loss:2.15581408684\n",
      "train loss:2.17811280975\n",
      "train loss:2.1748433228\n",
      "=== epoch:91, train acc:0.253333333333, test acc:0.226 ===\n",
      "train loss:2.17500109867\n",
      "train loss:2.12994177448\n",
      "train loss:2.20149887885\n",
      "=== epoch:92, train acc:0.25, test acc:0.2281 ===\n",
      "train loss:2.12851676467\n",
      "train loss:2.14090055781\n",
      "train loss:2.18613692133\n",
      "=== epoch:93, train acc:0.253333333333, test acc:0.2287 ===\n",
      "train loss:2.13036524506\n",
      "train loss:2.18105561317\n",
      "train loss:2.1792518226\n",
      "=== epoch:94, train acc:0.256666666667, test acc:0.2288 ===\n",
      "train loss:2.17368092854\n",
      "train loss:2.24400917893\n",
      "train loss:2.19016639393\n",
      "=== epoch:95, train acc:0.256666666667, test acc:0.2317 ===\n",
      "train loss:2.16627607577\n",
      "train loss:2.19266448802\n",
      "train loss:2.21093834657\n",
      "=== epoch:96, train acc:0.26, test acc:0.2347 ===\n",
      "train loss:2.11448327569\n",
      "train loss:2.19621134816\n",
      "train loss:2.13524597247\n",
      "=== epoch:97, train acc:0.27, test acc:0.2359 ===\n",
      "train loss:2.21012871179\n",
      "train loss:2.20219471089\n",
      "train loss:2.18611613671\n",
      "=== epoch:98, train acc:0.273333333333, test acc:0.2375 ===\n",
      "train loss:2.22059006709\n",
      "train loss:2.16140240791\n",
      "train loss:2.13394314086\n",
      "=== epoch:99, train acc:0.273333333333, test acc:0.238 ===\n",
      "train loss:2.13966284193\n",
      "train loss:2.1958661663\n",
      "train loss:2.1323797382\n",
      "=== epoch:100, train acc:0.276666666667, test acc:0.2398 ===\n",
      "train loss:2.10190308199\n",
      "train loss:2.14024316976\n",
      "train loss:2.20711735636\n",
      "=== epoch:101, train acc:0.28, test acc:0.2419 ===\n",
      "train loss:2.0955653489\n",
      "train loss:2.16806249453\n",
      "train loss:2.1190699673\n",
      "=== epoch:102, train acc:0.28, test acc:0.2426 ===\n",
      "train loss:2.14187380022\n",
      "train loss:2.11270719126\n",
      "train loss:2.14873852843\n",
      "=== epoch:103, train acc:0.283333333333, test acc:0.2437 ===\n",
      "train loss:2.1600406335\n",
      "train loss:2.14126328339\n",
      "train loss:2.06554013488\n",
      "=== epoch:104, train acc:0.286666666667, test acc:0.2437 ===\n",
      "train loss:2.18185933426\n",
      "train loss:2.16216579533\n",
      "train loss:2.13450532598\n",
      "=== epoch:105, train acc:0.286666666667, test acc:0.2445 ===\n",
      "train loss:2.15709895207\n",
      "train loss:2.20178269584\n",
      "train loss:2.1406411061\n",
      "=== epoch:106, train acc:0.29, test acc:0.2464 ===\n",
      "train loss:2.1194249324\n",
      "train loss:2.0909163928\n",
      "train loss:2.08794432381\n",
      "=== epoch:107, train acc:0.29, test acc:0.2469 ===\n",
      "train loss:2.15971455487\n",
      "train loss:2.15149312524\n",
      "train loss:2.08258532328\n",
      "=== epoch:108, train acc:0.293333333333, test acc:0.2478 ===\n",
      "train loss:2.14180754668\n",
      "train loss:2.17898509645\n",
      "train loss:2.11904972349\n",
      "=== epoch:109, train acc:0.3, test acc:0.2493 ===\n",
      "train loss:2.09957829377\n",
      "train loss:2.14886988876\n",
      "train loss:2.10661062117\n",
      "=== epoch:110, train acc:0.303333333333, test acc:0.2487 ===\n",
      "train loss:2.09349962376\n",
      "train loss:2.09944051232\n",
      "train loss:2.08578955394\n",
      "=== epoch:111, train acc:0.303333333333, test acc:0.2491 ===\n",
      "train loss:2.13826083527\n",
      "train loss:2.05779780227\n",
      "train loss:2.19798228846\n",
      "=== epoch:112, train acc:0.296666666667, test acc:0.2485 ===\n",
      "train loss:2.10061433976\n",
      "train loss:2.14540974493\n",
      "train loss:2.16259752673\n",
      "=== epoch:113, train acc:0.303333333333, test acc:0.2527 ===\n",
      "train loss:2.06718492763\n",
      "train loss:2.1482461539\n",
      "train loss:2.09692579591\n",
      "=== epoch:114, train acc:0.3, test acc:0.2553 ===\n",
      "train loss:2.0670649111\n",
      "train loss:2.05736679335\n",
      "train loss:2.08881600825\n",
      "=== epoch:115, train acc:0.3, test acc:0.2537 ===\n",
      "train loss:2.0946716757\n",
      "train loss:2.04488731015\n",
      "train loss:2.10679366948\n",
      "=== epoch:116, train acc:0.3, test acc:0.2543 ===\n",
      "train loss:2.05731135222\n",
      "train loss:2.09072650946\n",
      "train loss:2.15814670056\n",
      "=== epoch:117, train acc:0.3, test acc:0.2552 ===\n",
      "train loss:2.09564798138\n",
      "train loss:2.03688991266\n",
      "train loss:2.13768388922\n",
      "=== epoch:118, train acc:0.3, test acc:0.2565 ===\n",
      "train loss:2.09848540791\n",
      "train loss:2.10787464425\n",
      "train loss:2.13420743551\n",
      "=== epoch:119, train acc:0.303333333333, test acc:0.2566 ===\n",
      "train loss:2.13468930819\n",
      "train loss:2.01842414439\n",
      "train loss:2.14780483619\n",
      "=== epoch:120, train acc:0.303333333333, test acc:0.257 ===\n",
      "train loss:2.09016937534\n",
      "train loss:2.13802149179\n",
      "train loss:2.11133946619\n",
      "=== epoch:121, train acc:0.303333333333, test acc:0.2585 ===\n",
      "train loss:2.08483562326\n",
      "train loss:2.11395720398\n",
      "train loss:2.0128676913\n",
      "=== epoch:122, train acc:0.306666666667, test acc:0.2581 ===\n",
      "train loss:2.08658631012\n",
      "train loss:2.12906557617\n",
      "train loss:2.108540997\n",
      "=== epoch:123, train acc:0.31, test acc:0.2607 ===\n",
      "train loss:1.99482698648\n",
      "train loss:2.07450884624\n",
      "train loss:2.02848368709\n",
      "=== epoch:124, train acc:0.306666666667, test acc:0.2593 ===\n",
      "train loss:2.16195466597\n",
      "train loss:2.02305217853\n",
      "train loss:2.14550067877\n",
      "=== epoch:125, train acc:0.31, test acc:0.2605 ===\n",
      "train loss:2.1162353836\n",
      "train loss:2.03625628703\n",
      "train loss:2.12296866908\n",
      "=== epoch:126, train acc:0.313333333333, test acc:0.2625 ===\n",
      "train loss:2.01252833403\n",
      "train loss:2.13425118637\n",
      "train loss:2.10705845613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:127, train acc:0.316666666667, test acc:0.265 ===\n",
      "train loss:2.0337851916\n",
      "train loss:2.04557569002\n",
      "train loss:2.09365540541\n",
      "=== epoch:128, train acc:0.316666666667, test acc:0.2652 ===\n",
      "train loss:2.0286519579\n",
      "train loss:2.0473959889\n",
      "train loss:2.09138302656\n",
      "=== epoch:129, train acc:0.32, test acc:0.2647 ===\n",
      "train loss:2.02915867943\n",
      "train loss:1.89677071631\n",
      "train loss:1.98690391337\n",
      "=== epoch:130, train acc:0.316666666667, test acc:0.261 ===\n",
      "train loss:2.07234448238\n",
      "train loss:2.05231875877\n",
      "train loss:2.05236977285\n",
      "=== epoch:131, train acc:0.316666666667, test acc:0.2632 ===\n",
      "train loss:2.10223906571\n",
      "train loss:2.1046906643\n",
      "train loss:2.01323235349\n",
      "=== epoch:132, train acc:0.32, test acc:0.2672 ===\n",
      "train loss:2.0549740132\n",
      "train loss:2.00890678601\n",
      "train loss:2.01654505474\n",
      "=== epoch:133, train acc:0.326666666667, test acc:0.27 ===\n",
      "train loss:1.98356098639\n",
      "train loss:2.10073668063\n",
      "train loss:2.05355934991\n",
      "=== epoch:134, train acc:0.323333333333, test acc:0.2705 ===\n",
      "train loss:2.05613312783\n",
      "train loss:1.95298442474\n",
      "train loss:1.99266408305\n",
      "=== epoch:135, train acc:0.32, test acc:0.2707 ===\n",
      "train loss:1.95420894564\n",
      "train loss:2.0188253925\n",
      "train loss:1.97978794634\n",
      "=== epoch:136, train acc:0.323333333333, test acc:0.2716 ===\n",
      "train loss:2.01138012363\n",
      "train loss:2.06301325515\n",
      "train loss:2.07780519271\n",
      "=== epoch:137, train acc:0.32, test acc:0.2717 ===\n",
      "train loss:1.9992472901\n",
      "train loss:2.01576648376\n",
      "train loss:2.04048696556\n",
      "=== epoch:138, train acc:0.323333333333, test acc:0.2717 ===\n",
      "train loss:2.0622748669\n",
      "train loss:2.02261028992\n",
      "train loss:2.18798904518\n",
      "=== epoch:139, train acc:0.323333333333, test acc:0.275 ===\n",
      "train loss:2.04935832683\n",
      "train loss:2.0624461861\n",
      "train loss:1.90570282575\n",
      "=== epoch:140, train acc:0.323333333333, test acc:0.278 ===\n",
      "train loss:2.03929872252\n",
      "train loss:2.0021631309\n",
      "train loss:1.96376963589\n",
      "=== epoch:141, train acc:0.326666666667, test acc:0.2786 ===\n",
      "train loss:2.05756486661\n",
      "train loss:2.02906606317\n",
      "train loss:2.04252466627\n",
      "=== epoch:142, train acc:0.33, test acc:0.2784 ===\n",
      "train loss:2.01205016631\n",
      "train loss:2.06344404315\n",
      "train loss:2.00807874228\n",
      "=== epoch:143, train acc:0.33, test acc:0.2759 ===\n",
      "train loss:1.96701340593\n",
      "train loss:1.93828281401\n",
      "train loss:2.02041481031\n",
      "=== epoch:144, train acc:0.33, test acc:0.2762 ===\n",
      "train loss:1.96560974952\n",
      "train loss:1.97338073065\n",
      "train loss:2.03680862764\n",
      "=== epoch:145, train acc:0.33, test acc:0.2752 ===\n",
      "train loss:2.04886610258\n",
      "train loss:1.97590265051\n",
      "train loss:2.04000605691\n",
      "=== epoch:146, train acc:0.33, test acc:0.2773 ===\n",
      "train loss:2.03207589048\n",
      "train loss:1.99047493334\n",
      "train loss:1.93234283137\n",
      "=== epoch:147, train acc:0.336666666667, test acc:0.2783 ===\n",
      "train loss:2.02760274686\n",
      "train loss:2.03176788656\n",
      "train loss:2.03175140411\n",
      "=== epoch:148, train acc:0.343333333333, test acc:0.2806 ===\n",
      "train loss:2.01625121813\n",
      "train loss:2.06597405722\n",
      "train loss:2.06350809291\n",
      "=== epoch:149, train acc:0.346666666667, test acc:0.2828 ===\n",
      "train loss:1.84934975655\n",
      "train loss:1.98192927139\n",
      "train loss:2.0843694347\n",
      "=== epoch:150, train acc:0.35, test acc:0.2843 ===\n",
      "train loss:2.01533246142\n",
      "train loss:1.96783193969\n",
      "train loss:1.94570839394\n",
      "=== epoch:151, train acc:0.346666666667, test acc:0.285 ===\n",
      "train loss:1.97234212064\n",
      "train loss:1.94196503952\n",
      "train loss:2.04466522437\n",
      "=== epoch:152, train acc:0.346666666667, test acc:0.2846 ===\n",
      "train loss:1.81586402002\n",
      "train loss:1.9208861803\n",
      "train loss:1.953679861\n",
      "=== epoch:153, train acc:0.353333333333, test acc:0.2902 ===\n",
      "train loss:2.00269906498\n",
      "train loss:1.87362489647\n",
      "train loss:2.07127881991\n",
      "=== epoch:154, train acc:0.353333333333, test acc:0.2911 ===\n",
      "train loss:1.94837210199\n",
      "train loss:1.93169868777\n",
      "train loss:1.79106007699\n",
      "=== epoch:155, train acc:0.353333333333, test acc:0.2902 ===\n",
      "train loss:2.04138669385\n",
      "train loss:1.96349462414\n",
      "train loss:1.8502897607\n",
      "=== epoch:156, train acc:0.353333333333, test acc:0.2914 ===\n",
      "train loss:2.0628648939\n",
      "train loss:1.93555113515\n",
      "train loss:2.01714545206\n",
      "=== epoch:157, train acc:0.353333333333, test acc:0.2985 ===\n",
      "train loss:1.93178289115\n",
      "train loss:2.02444828331\n",
      "train loss:1.93325124727\n",
      "=== epoch:158, train acc:0.356666666667, test acc:0.2996 ===\n",
      "train loss:1.90183036318\n",
      "train loss:1.93267569314\n",
      "train loss:1.93387403475\n",
      "=== epoch:159, train acc:0.35, test acc:0.2979 ===\n",
      "train loss:1.93027804675\n",
      "train loss:2.03954571635\n",
      "train loss:1.75287343548\n",
      "=== epoch:160, train acc:0.35, test acc:0.2975 ===\n",
      "train loss:1.92979360704\n",
      "train loss:1.97691893841\n",
      "train loss:1.99012566397\n",
      "=== epoch:161, train acc:0.353333333333, test acc:0.2997 ===\n",
      "train loss:1.95090382705\n",
      "train loss:2.00373876518\n",
      "train loss:1.97664619107\n",
      "=== epoch:162, train acc:0.356666666667, test acc:0.3026 ===\n",
      "train loss:1.93692375815\n",
      "train loss:1.93677149394\n",
      "train loss:1.95302271917\n",
      "=== epoch:163, train acc:0.356666666667, test acc:0.305 ===\n",
      "train loss:1.85922366704\n",
      "train loss:1.91523514264\n",
      "train loss:1.95974581634\n",
      "=== epoch:164, train acc:0.36, test acc:0.3036 ===\n",
      "train loss:1.98492222571\n",
      "train loss:1.90555813018\n",
      "train loss:1.89409788158\n",
      "=== epoch:165, train acc:0.37, test acc:0.3066 ===\n",
      "train loss:1.89943108535\n",
      "train loss:1.96960434984\n",
      "train loss:1.9418503539\n",
      "=== epoch:166, train acc:0.373333333333, test acc:0.3114 ===\n",
      "train loss:1.96590302028\n",
      "train loss:1.92021865876\n",
      "train loss:1.94735098714\n",
      "=== epoch:167, train acc:0.373333333333, test acc:0.3167 ===\n",
      "train loss:1.94254598267\n",
      "train loss:1.988434849\n",
      "train loss:1.88975528184\n",
      "=== epoch:168, train acc:0.373333333333, test acc:0.3198 ===\n",
      "train loss:1.94178772969\n",
      "train loss:2.01752095006\n",
      "train loss:1.84815350492\n",
      "=== epoch:169, train acc:0.373333333333, test acc:0.3196 ===\n",
      "train loss:1.98669962216\n",
      "train loss:2.04075893346\n",
      "train loss:1.83637663366\n",
      "=== epoch:170, train acc:0.403333333333, test acc:0.3296 ===\n",
      "train loss:1.80037382599\n",
      "train loss:2.04532026691\n",
      "train loss:1.87769134612\n",
      "=== epoch:171, train acc:0.406666666667, test acc:0.3325 ===\n",
      "train loss:1.94144944842\n",
      "train loss:1.93063232577\n",
      "train loss:1.86348925845\n",
      "=== epoch:172, train acc:0.403333333333, test acc:0.3306 ===\n",
      "train loss:1.77460927858\n",
      "train loss:1.90130941986\n",
      "train loss:1.833681645\n",
      "=== epoch:173, train acc:0.413333333333, test acc:0.333 ===\n",
      "train loss:1.93153375355\n",
      "train loss:1.99288244182\n",
      "train loss:1.90145654584\n",
      "=== epoch:174, train acc:0.42, test acc:0.3376 ===\n",
      "train loss:1.95318230094\n",
      "train loss:1.97381757044\n",
      "train loss:1.82752010918\n",
      "=== epoch:175, train acc:0.426666666667, test acc:0.3399 ===\n",
      "train loss:1.90985908047\n",
      "train loss:1.95018035487\n",
      "train loss:1.88028973524\n",
      "=== epoch:176, train acc:0.42, test acc:0.3363 ===\n",
      "train loss:2.05869843294\n",
      "train loss:1.95099159215\n",
      "train loss:1.88454496812\n",
      "=== epoch:177, train acc:0.423333333333, test acc:0.3401 ===\n",
      "train loss:1.87443744712\n",
      "train loss:2.04812178165\n",
      "train loss:1.97845911795\n",
      "=== epoch:178, train acc:0.433333333333, test acc:0.3477 ===\n",
      "train loss:1.93664548281\n",
      "train loss:1.87187156417\n",
      "train loss:1.92904945632\n",
      "=== epoch:179, train acc:0.436666666667, test acc:0.3495 ===\n",
      "train loss:1.92602260061\n",
      "train loss:1.85329312801\n",
      "train loss:1.95389584546\n",
      "=== epoch:180, train acc:0.436666666667, test acc:0.3472 ===\n",
      "train loss:1.87706135378\n",
      "train loss:1.88197646785\n",
      "train loss:1.90542937268\n",
      "=== epoch:181, train acc:0.433333333333, test acc:0.3511 ===\n",
      "train loss:1.97540138823\n",
      "train loss:1.92760974596\n",
      "train loss:1.9473213531\n",
      "=== epoch:182, train acc:0.436666666667, test acc:0.3583 ===\n",
      "train loss:1.9620812644\n",
      "train loss:1.95388146271\n",
      "train loss:1.9756227392\n",
      "=== epoch:183, train acc:0.443333333333, test acc:0.3586 ===\n",
      "train loss:1.87879752405\n",
      "train loss:1.88406763716\n",
      "train loss:1.96388864782\n",
      "=== epoch:184, train acc:0.446666666667, test acc:0.3624 ===\n",
      "train loss:1.89498870521\n",
      "train loss:1.87308032772\n",
      "train loss:1.89719720643\n",
      "=== epoch:185, train acc:0.446666666667, test acc:0.3624 ===\n",
      "train loss:1.93069445303\n",
      "train loss:1.89643110132\n",
      "train loss:1.89616019171\n",
      "=== epoch:186, train acc:0.443333333333, test acc:0.3707 ===\n",
      "train loss:1.92727057874\n",
      "train loss:1.78683983797\n",
      "train loss:1.80444034168\n",
      "=== epoch:187, train acc:0.446666666667, test acc:0.3682 ===\n",
      "train loss:1.77453742039\n",
      "train loss:1.79554657924\n",
      "train loss:1.92364621555\n",
      "=== epoch:188, train acc:0.453333333333, test acc:0.3694 ===\n",
      "train loss:2.00299791773\n",
      "train loss:2.01609844649\n",
      "train loss:1.89605195348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:189, train acc:0.46, test acc:0.3787 ===\n",
      "train loss:1.91501722381\n",
      "train loss:1.76836180123\n",
      "train loss:1.803704267\n",
      "=== epoch:190, train acc:0.473333333333, test acc:0.3806 ===\n",
      "train loss:1.86062687801\n",
      "train loss:1.86553443791\n",
      "train loss:1.69857293681\n",
      "=== epoch:191, train acc:0.466666666667, test acc:0.377 ===\n",
      "train loss:1.96540467419\n",
      "train loss:1.89800499873\n",
      "train loss:1.82201151039\n",
      "=== epoch:192, train acc:0.463333333333, test acc:0.3734 ===\n",
      "train loss:1.81407569401\n",
      "train loss:1.85699182688\n",
      "train loss:1.88307140966\n",
      "=== epoch:193, train acc:0.46, test acc:0.3701 ===\n",
      "train loss:1.90754758025\n",
      "train loss:1.83687998271\n",
      "train loss:1.82102953442\n",
      "=== epoch:194, train acc:0.46, test acc:0.3674 ===\n",
      "train loss:1.88565653032\n",
      "train loss:1.81996621113\n",
      "train loss:1.93871553147\n",
      "=== epoch:195, train acc:0.45, test acc:0.365 ===\n",
      "train loss:1.87909825193\n",
      "train loss:1.90633802528\n",
      "train loss:1.89097526553\n",
      "=== epoch:196, train acc:0.463333333333, test acc:0.3736 ===\n",
      "train loss:1.93732997848\n",
      "train loss:1.87672781827\n",
      "train loss:2.0009955688\n",
      "=== epoch:197, train acc:0.463333333333, test acc:0.3784 ===\n",
      "train loss:1.88641520607\n",
      "train loss:1.87184264268\n",
      "train loss:1.77115454414\n",
      "=== epoch:198, train acc:0.473333333333, test acc:0.3842 ===\n",
      "train loss:1.79215600615\n",
      "train loss:1.79562150964\n",
      "train loss:1.80836365371\n",
      "=== epoch:199, train acc:0.48, test acc:0.3852 ===\n",
      "train loss:1.97164974153\n",
      "train loss:1.83464325995\n",
      "train loss:1.81298813767\n",
      "=== epoch:200, train acc:0.493333333333, test acc:0.3898 ===\n",
      "train loss:1.89094489768\n",
      "train loss:1.87982652589\n",
      "train loss:1.80272277861\n",
      "=== epoch:201, train acc:0.473333333333, test acc:0.3893 ===\n",
      "train loss:1.84000045686\n",
      "train loss:1.78207125398\n",
      "train loss:1.74180743683\n",
      "=== epoch:202, train acc:0.476666666667, test acc:0.391 ===\n",
      "train loss:1.79050413296\n",
      "train loss:1.89706766128\n",
      "train loss:1.7740150693\n",
      "=== epoch:203, train acc:0.476666666667, test acc:0.3984 ===\n",
      "train loss:1.77803666304\n",
      "train loss:1.81842916325\n",
      "train loss:1.88589395825\n",
      "=== epoch:204, train acc:0.486666666667, test acc:0.4006 ===\n",
      "train loss:1.77884593418\n",
      "train loss:1.76006303764\n",
      "train loss:1.77671928483\n",
      "=== epoch:205, train acc:0.483333333333, test acc:0.4001 ===\n",
      "train loss:1.84033326793\n",
      "train loss:1.81502106342\n",
      "train loss:1.80554555516\n",
      "=== epoch:206, train acc:0.49, test acc:0.4012 ===\n",
      "train loss:1.84513653947\n",
      "train loss:1.74939414027\n",
      "train loss:1.73362923003\n",
      "=== epoch:207, train acc:0.486666666667, test acc:0.4064 ===\n",
      "train loss:1.8140287417\n",
      "train loss:1.72623688889\n",
      "train loss:1.71374483699\n",
      "=== epoch:208, train acc:0.48, test acc:0.4008 ===\n",
      "train loss:1.86189475291\n",
      "train loss:1.75884707806\n",
      "train loss:1.80025117593\n",
      "=== epoch:209, train acc:0.486666666667, test acc:0.4012 ===\n",
      "train loss:1.8784069699\n",
      "train loss:1.79815689315\n",
      "train loss:1.8349769411\n",
      "=== epoch:210, train acc:0.49, test acc:0.404 ===\n",
      "train loss:1.77643266977\n",
      "train loss:1.85707256049\n",
      "train loss:1.74160248078\n",
      "=== epoch:211, train acc:0.493333333333, test acc:0.4104 ===\n",
      "train loss:1.84024668148\n",
      "train loss:1.6537304182\n",
      "train loss:1.79701574551\n",
      "=== epoch:212, train acc:0.5, test acc:0.4166 ===\n",
      "train loss:1.74239847046\n",
      "train loss:1.87364098623\n",
      "train loss:1.73836982583\n",
      "=== epoch:213, train acc:0.51, test acc:0.4183 ===\n",
      "train loss:1.7355020091\n",
      "train loss:1.74573512441\n",
      "train loss:1.82558935466\n",
      "=== epoch:214, train acc:0.496666666667, test acc:0.4173 ===\n",
      "train loss:1.83711769624\n",
      "train loss:1.72561197567\n",
      "train loss:1.71488381103\n",
      "=== epoch:215, train acc:0.5, test acc:0.4199 ===\n",
      "train loss:1.85396909323\n",
      "train loss:1.77155815726\n",
      "train loss:1.74731173679\n",
      "=== epoch:216, train acc:0.503333333333, test acc:0.4221 ===\n",
      "train loss:1.8531369061\n",
      "train loss:1.8525604876\n",
      "train loss:1.92798286838\n",
      "=== epoch:217, train acc:0.516666666667, test acc:0.4191 ===\n",
      "train loss:1.75434410025\n",
      "train loss:1.77482066911\n",
      "train loss:1.81266186294\n",
      "=== epoch:218, train acc:0.516666666667, test acc:0.4187 ===\n",
      "train loss:1.77865411498\n",
      "train loss:1.86793342583\n",
      "train loss:1.76634797796\n",
      "=== epoch:219, train acc:0.513333333333, test acc:0.4181 ===\n",
      "train loss:1.72591974645\n",
      "train loss:1.77371453119\n",
      "train loss:1.88153080043\n",
      "=== epoch:220, train acc:0.523333333333, test acc:0.4233 ===\n",
      "train loss:1.9004205827\n",
      "train loss:1.86359397897\n",
      "train loss:1.82303938418\n",
      "=== epoch:221, train acc:0.52, test acc:0.4226 ===\n",
      "train loss:1.78948378817\n",
      "train loss:1.7133231379\n",
      "train loss:1.82517381544\n",
      "=== epoch:222, train acc:0.526666666667, test acc:0.4304 ===\n",
      "train loss:1.85109327103\n",
      "train loss:1.84800440594\n",
      "train loss:1.64329014913\n",
      "=== epoch:223, train acc:0.52, test acc:0.426 ===\n",
      "train loss:1.79944122616\n",
      "train loss:1.56175734815\n",
      "train loss:1.75854307804\n",
      "=== epoch:224, train acc:0.52, test acc:0.4231 ===\n",
      "train loss:1.69204676009\n",
      "train loss:1.78006411859\n",
      "train loss:1.76370170889\n",
      "=== epoch:225, train acc:0.52, test acc:0.4204 ===\n",
      "train loss:1.68107420814\n",
      "train loss:1.64280676786\n",
      "train loss:1.77581353084\n",
      "=== epoch:226, train acc:0.526666666667, test acc:0.4205 ===\n",
      "train loss:1.73623735641\n",
      "train loss:1.80027963176\n",
      "train loss:1.61593188548\n",
      "=== epoch:227, train acc:0.523333333333, test acc:0.4193 ===\n",
      "train loss:1.65164448449\n",
      "train loss:1.65122760346\n",
      "train loss:1.75774375429\n",
      "=== epoch:228, train acc:0.526666666667, test acc:0.4187 ===\n",
      "train loss:1.79189721362\n",
      "train loss:1.57682182055\n",
      "train loss:1.68213453359\n",
      "=== epoch:229, train acc:0.536666666667, test acc:0.4192 ===\n",
      "train loss:1.74441667537\n",
      "train loss:1.6167189255\n",
      "train loss:1.67221855533\n",
      "=== epoch:230, train acc:0.526666666667, test acc:0.4179 ===\n",
      "train loss:1.8003202953\n",
      "train loss:1.77077684016\n",
      "train loss:1.6718710384\n",
      "=== epoch:231, train acc:0.523333333333, test acc:0.4176 ===\n",
      "train loss:1.64115734293\n",
      "train loss:1.63869580938\n",
      "train loss:1.74465393088\n",
      "=== epoch:232, train acc:0.516666666667, test acc:0.4195 ===\n",
      "train loss:1.70641552886\n",
      "train loss:1.69686365482\n",
      "train loss:1.68909662946\n",
      "=== epoch:233, train acc:0.516666666667, test acc:0.4196 ===\n",
      "train loss:1.66875427821\n",
      "train loss:1.77528979967\n",
      "train loss:1.68566968927\n",
      "=== epoch:234, train acc:0.516666666667, test acc:0.4207 ===\n",
      "train loss:1.82475615024\n",
      "train loss:1.76805024631\n",
      "train loss:1.61643155774\n",
      "=== epoch:235, train acc:0.52, test acc:0.422 ===\n",
      "train loss:1.61037921504\n",
      "train loss:1.74647896035\n",
      "train loss:1.70611614424\n",
      "=== epoch:236, train acc:0.513333333333, test acc:0.4197 ===\n",
      "train loss:1.73335042557\n",
      "train loss:1.81660072342\n",
      "train loss:1.69773143683\n",
      "=== epoch:237, train acc:0.52, test acc:0.421 ===\n",
      "train loss:1.6374405339\n",
      "train loss:1.67456340822\n",
      "train loss:1.65663066541\n",
      "=== epoch:238, train acc:0.533333333333, test acc:0.4223 ===\n",
      "train loss:1.58711474568\n",
      "train loss:1.58321794298\n",
      "train loss:1.81410806598\n",
      "=== epoch:239, train acc:0.533333333333, test acc:0.4221 ===\n",
      "train loss:1.76679690665\n",
      "train loss:1.77036153786\n",
      "train loss:1.58785164767\n",
      "=== epoch:240, train acc:0.53, test acc:0.424 ===\n",
      "train loss:1.78457636536\n",
      "train loss:1.68306768334\n",
      "train loss:1.81442665412\n",
      "=== epoch:241, train acc:0.53, test acc:0.4229 ===\n",
      "train loss:1.70477552737\n",
      "train loss:1.75524753602\n",
      "train loss:1.76067272668\n",
      "=== epoch:242, train acc:0.533333333333, test acc:0.4225 ===\n",
      "train loss:1.68813175566\n",
      "train loss:1.67875533606\n",
      "train loss:1.77764002617\n",
      "=== epoch:243, train acc:0.536666666667, test acc:0.4263 ===\n",
      "train loss:1.66263292278\n",
      "train loss:1.68584704071\n",
      "train loss:1.50982226399\n",
      "=== epoch:244, train acc:0.516666666667, test acc:0.4218 ===\n",
      "train loss:1.65495566471\n",
      "train loss:1.7635932985\n",
      "train loss:1.63677213176\n",
      "=== epoch:245, train acc:0.52, test acc:0.4213 ===\n",
      "train loss:1.67970580586\n",
      "train loss:1.64947447496\n",
      "train loss:1.70400132247\n",
      "=== epoch:246, train acc:0.52, test acc:0.4204 ===\n",
      "train loss:1.73065858076\n",
      "train loss:1.70322499921\n",
      "train loss:1.71733489043\n",
      "=== epoch:247, train acc:0.513333333333, test acc:0.4175 ===\n",
      "train loss:1.68092840533\n",
      "train loss:1.70935262566\n",
      "train loss:1.73182178691\n",
      "=== epoch:248, train acc:0.513333333333, test acc:0.4177 ===\n",
      "train loss:1.78290930565\n",
      "train loss:1.55278388861\n",
      "train loss:1.54812116707\n",
      "=== epoch:249, train acc:0.516666666667, test acc:0.42 ===\n",
      "train loss:1.8161177041\n",
      "train loss:1.77080624198\n",
      "train loss:1.62721153213\n",
      "=== epoch:250, train acc:0.516666666667, test acc:0.4239 ===\n",
      "train loss:1.41807743266\n",
      "train loss:1.6401171523\n",
      "train loss:1.61645620933\n",
      "=== epoch:251, train acc:0.513333333333, test acc:0.4277 ===\n",
      "train loss:1.60812892986\n",
      "train loss:1.64543720411\n",
      "train loss:1.63640665101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:252, train acc:0.506666666667, test acc:0.4238 ===\n",
      "train loss:1.66807349258\n",
      "train loss:1.59207134809\n",
      "train loss:1.51995170435\n",
      "=== epoch:253, train acc:0.5, test acc:0.4259 ===\n",
      "train loss:1.70392370025\n",
      "train loss:1.70567256459\n",
      "train loss:1.71757880994\n",
      "=== epoch:254, train acc:0.513333333333, test acc:0.4271 ===\n",
      "train loss:1.52271497735\n",
      "train loss:1.61645076203\n",
      "train loss:1.53941565111\n",
      "=== epoch:255, train acc:0.516666666667, test acc:0.4297 ===\n",
      "train loss:1.63629314464\n",
      "train loss:1.73567137534\n",
      "train loss:1.53382813963\n",
      "=== epoch:256, train acc:0.52, test acc:0.4314 ===\n",
      "train loss:1.73793579263\n",
      "train loss:1.57593686194\n",
      "train loss:1.53317827006\n",
      "=== epoch:257, train acc:0.516666666667, test acc:0.4316 ===\n",
      "train loss:1.67766065434\n",
      "train loss:1.60150423302\n",
      "train loss:1.6358519765\n",
      "=== epoch:258, train acc:0.51, test acc:0.4287 ===\n",
      "train loss:1.68660385411\n",
      "train loss:1.72057285934\n",
      "train loss:1.5986544628\n",
      "=== epoch:259, train acc:0.516666666667, test acc:0.4308 ===\n",
      "train loss:1.63025195164\n",
      "train loss:1.58441001036\n",
      "train loss:1.74957069367\n",
      "=== epoch:260, train acc:0.516666666667, test acc:0.4329 ===\n",
      "train loss:1.71784207074\n",
      "train loss:1.55795323847\n",
      "train loss:1.51487153179\n",
      "=== epoch:261, train acc:0.503333333333, test acc:0.4308 ===\n",
      "train loss:1.65434704251\n",
      "train loss:1.62877354228\n",
      "train loss:1.58959455658\n",
      "=== epoch:262, train acc:0.51, test acc:0.434 ===\n",
      "train loss:1.46504769193\n",
      "train loss:1.6849935321\n",
      "train loss:1.54783852541\n",
      "=== epoch:263, train acc:0.503333333333, test acc:0.4331 ===\n",
      "train loss:1.70263951835\n",
      "train loss:1.57798707664\n",
      "train loss:1.64201544828\n",
      "=== epoch:264, train acc:0.506666666667, test acc:0.4337 ===\n",
      "train loss:1.61795757563\n",
      "train loss:1.59466924936\n",
      "train loss:1.60637229944\n",
      "=== epoch:265, train acc:0.503333333333, test acc:0.4367 ===\n",
      "train loss:1.6580633895\n",
      "train loss:1.3910231292\n",
      "train loss:1.53198613989\n",
      "=== epoch:266, train acc:0.506666666667, test acc:0.4388 ===\n",
      "train loss:1.55277893748\n",
      "train loss:1.53063235692\n",
      "train loss:1.60700950145\n",
      "=== epoch:267, train acc:0.5, test acc:0.4367 ===\n",
      "train loss:1.4819177727\n",
      "train loss:1.59545863122\n",
      "train loss:1.51472551688\n",
      "=== epoch:268, train acc:0.5, test acc:0.439 ===\n",
      "train loss:1.59917087786\n",
      "train loss:1.56786548816\n",
      "train loss:1.66916717506\n",
      "=== epoch:269, train acc:0.503333333333, test acc:0.4358 ===\n",
      "train loss:1.50997583308\n",
      "train loss:1.56722007255\n",
      "train loss:1.43692702768\n",
      "=== epoch:270, train acc:0.506666666667, test acc:0.4387 ===\n",
      "train loss:1.53987130556\n",
      "train loss:1.60962145994\n",
      "train loss:1.59493255951\n",
      "=== epoch:271, train acc:0.506666666667, test acc:0.4392 ===\n",
      "train loss:1.62175509195\n",
      "train loss:1.49313125095\n",
      "train loss:1.57221367997\n",
      "=== epoch:272, train acc:0.506666666667, test acc:0.4438 ===\n",
      "train loss:1.51993930098\n",
      "train loss:1.62812615544\n",
      "train loss:1.46481543737\n",
      "=== epoch:273, train acc:0.506666666667, test acc:0.4428 ===\n",
      "train loss:1.65026659391\n",
      "train loss:1.51982127208\n",
      "train loss:1.53965492902\n",
      "=== epoch:274, train acc:0.51, test acc:0.4432 ===\n",
      "train loss:1.53008585852\n",
      "train loss:1.55354514671\n",
      "train loss:1.64977382207\n",
      "=== epoch:275, train acc:0.516666666667, test acc:0.4452 ===\n",
      "train loss:1.58150346331\n",
      "train loss:1.6335521385\n",
      "train loss:1.54748789629\n",
      "=== epoch:276, train acc:0.513333333333, test acc:0.4472 ===\n",
      "train loss:1.60667967986\n",
      "train loss:1.5801277336\n",
      "train loss:1.50953108075\n",
      "=== epoch:277, train acc:0.516666666667, test acc:0.4535 ===\n",
      "train loss:1.58903451764\n",
      "train loss:1.51877646886\n",
      "train loss:1.48541448929\n",
      "=== epoch:278, train acc:0.52, test acc:0.4505 ===\n",
      "train loss:1.60418535052\n",
      "train loss:1.37056477864\n",
      "train loss:1.51549698791\n",
      "=== epoch:279, train acc:0.52, test acc:0.4541 ===\n",
      "train loss:1.58021065483\n",
      "train loss:1.45097043819\n",
      "train loss:1.52534456005\n",
      "=== epoch:280, train acc:0.516666666667, test acc:0.4473 ===\n",
      "train loss:1.64828288898\n",
      "train loss:1.59375801028\n",
      "train loss:1.47456502822\n",
      "=== epoch:281, train acc:0.52, test acc:0.4495 ===\n",
      "train loss:1.54641336575\n",
      "train loss:1.51447788329\n",
      "train loss:1.63764005962\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 過学習を再現するために、学習データを削減\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# Dropuoutの有無、割り合いの設定 ========================\n",
    "use_dropout = True  # Dropoutなしのときの場合はFalseに\n",
    "dropout_ratio = 0.2\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
    "\n",
    "# グラフの描画==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
